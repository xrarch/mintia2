//
// Implements system memory pool management.
//

#INCLUDE "Mi.hjk"

#DEFINE MI_FULL_BLOCKS_PER_EXTENSION [(RTL_PAGE_SIZE / MI_BLOCK_MAX_SIZE - 1)]

#DEFINE MI_BLOCK_SWIFT_MAGIC 0xC0
#DEFINE MI_BLOCK_FREE_MAGIC 0xE4
#DEFINE MI_BLOCK_INVALID_MAGIC 0xA5

STRUCT MiBlock
    BucketIndex : UBYTE,
    LastSize : UBYTE,
    Magic : UBYTE,
    Size : UBYTE,

    Tag : ULONG,

#IF ( == RTL_ALIGN_SHIFT 4 )
    AlignToSixteen1 : ULONG,
    AlignToSixteen2 : ULONG,
#END

END

STRUCT MiFreeBlock
    Block : MiBlock,

    LogEntry : RtlListEntry,
    SizeEntry : RtlListEntry,
END

STRUCT MiPoolExtension
    Pool : ^MiPool,
    References : ULONG,
END

#DEFINE MI_POOL_FLAG_USE_PRIVILEGED_MEM 1

FN MiAllocateExtensionSpace (
    IN node : ^MiNode,
) : ^MiPoolExtension

    // Allocate space for a pool extension.

    printed := FALSE

    WHILE TRUE DO
        ext := CAST MiAllocateChunkSpace (
            &node^.PoolSpace, // chunkspace
        ) TO ^MiPoolExtension

        IF ext THEN
            RETURN ext
        END

        // Uh-oh! If we're out of pool space there isn't much good to be done.
        // Sleep for 500ms and retry.

        IF NOT printed THEN
            RtlPrint ( "Out of pool space!\n" )

            printed = TRUE
        END

        interval : RtlUquad
        
        RtlSetUquadToUlong ( &interval, 500 )

        KeSleep (
            &interval, // interval
            KE_KERNEL_MODE, // waitmode
            KE_UNALERTABLE, // alertable
        )
    END
END

FN MiFreeExtensionSpace (
    IN node : ^MiNode,
    IN ext : ^MiPoolExtension,
    IN flush : UWORD,
)

    // Free the virtual space for the pool extension.

    pte := MiPteAddress ( ext )

    // Clear the PTE, makes it more likely we'll see it if a bad pointer within
    // this chunk is used later. Due to the TB it's not 100% though.

    pte[0] = MI_INVALID_KERNEL_PTE

#IF BLD_CHK
    // Make it even more likely by at least flushing this page on the current
    // processor.

    KeFlushMyTbAddress ( ext )
#END

    MiFreeChunkSpace (
        &node^.PoolSpace, // chunkspace
        ext, // ptr
        flush, // flush
    )
END

FN (MiPoolReturnMemoryF) MiFreeNonpagedMemory (
    IN partition : ^MiPartition,
    IN ptr : ^VOID,
    IN pages : UWORD,
)

    // Free the page frames within this extension and return it to nonpaged
    // space. The space allocator will deal with flushing the TB.

    pte := MiPteAddress ( ptr )

    i := 0

    WHILE i < pages DO
        ptecontents := pte[i]

        MiFreePage (
            partition, // partition
            MiPfnToPfe ( MiPfnFromPte ( ptecontents ) ), // pfe
        )

        pte[i] = MI_INVALID_KERNEL_PTE

#IF BLD_CHK
        KeFlushMyTbAddress ( ptr )

        ptr += RTL_PAGE_SIZE
#END

        i += 1
    END

    // Uncharge the commit.

    MmUnchargeCommit (
        partition, // partition
        pages, // pages
    )
END

FN (MiPoolGetMemoryF) MiGetNonpagedMemory (
    IN partition : ^MiPartition,
    IN ptr : ^VOID,
    IN pages : UWORD,
    IN low : UWORD,
    IN wait : UWORD,
) : UWORD
    
    // Allocate an extension for nonpaged pool. If the required page frame can't
    // be immediately allocated, return NULLPTR. Caller will decide whether to
    // wait for free memory and retry the allocation, or give up.

    // First charge commit for these pages.

    MmChargeCommit (
        partition, // partition
        pages, // pages
        FALSE, // wait
    )

    i := 0
    pte := MiPteAddress ( ptr )

    WHILE i < pages DO
        // Allocate a page frame to place here.

@Retry

        pfe := MiAllocatePage (
            partition, // partition
            FALSE, // zeroed
            low, // low
            MiVirtualAddressColor ( ptr ), // color
        )

        IF NOT pfe THEN
            IF wait THEN
                // Caller said we should wait here for more pages.

                MiWaitForPages (
                    partition, // partition
                    low, // low
                )

                GOTO Retry
            END

            // Free everything we just did.

            IF i THEN
                MiFreeNonpagedMemory (
                    partition, // partition
                    ptr, // ptr
                    i, // pages
                )
            END

            RETURN FALSE
        END

        // Map the page frame.
        // Note that no TB entry exists for this page in any CPU if we've been
        // given it by the pool space allocator.

        pte[i] = MiBuildPoolPte ( MiPfeToPfn ( pfe ) )

        i += 1
        ptr += RTL_PAGE_SIZE
    END

    RETURN TRUE
END

FN (MiPoolReturnMemoryF) MiFreePagedMemory (
    IN partition : ^MiPartition,
    IN ptr : ^VOID,
    IN pages : UWORD,
)

    // Free an extension for paged pool.

    KeCrash ( "NYI MiFreePagedMemory\n" )
END

FN (MiPoolGetMemoryF) MiGetPagedMemory (
    IN partition : ^MiPartition,
    IN ptr : ^VOID,
    IN pages : UWORD,
    IN low : UWORD,
    IN wait : UWORD,
) : UWORD
    
    // Allocate an extension for paged pool.

    KeCrash ( "NYI MiGetPagedMemory\n" )
END

#MACRO MiFindPoolExtension ( ptr ) [
    (CAST (ptr) & RTL_PAGE_NUMBER_MASK TO ^MiPoolExtension)
]

#SECTION "INITtext"
FN MiInitializePool (
    IN pool : ^MiPool,
    IN node : ^MiNode,
)

    // Initialize a pool.

    KeInitializeLock ( &pool^.Lock )

    pool^.BytesUsedInternally = 0
    pool^.BytesUsedExternally = 0
    pool^.BytesUsedPeak = 0
    pool^.Flags = 0
    pool^.Partition = &node^.Partition
    pool^.Node = node

    i := 0

    WHILE i < MI_BLOCK_LOG_BUCKETS DO
        RtlInitializeList ( &pool^.LogListHeads[i] )

        i += 1
    END

    i = 0

    WHILE i < MI_BLOCK_SIZE_BUCKETS DO
        RtlInitializeList ( &pool^.SizeListHeads[i] )

        i += 1
    END
END

#SECTION "INITtext"
FN MiInitializeNonpagedPool (
    IN pool : ^MiPool,
    IN node : ^MiNode,
)

    // Initialize a nonpaged pool.

    MiInitializePool (
        pool, // pool
        node, // node
    )

    pool^.GetMemory = &MiGetNonpagedMemory
    pool^.ReturnMemory = &MiFreeNonpagedMemory
END

#SECTION "INITtext"
FN MiInitializePagedPool (
    IN pool : ^MiPool,
    IN node : ^MiNode,
)

    // Initialize a paged pool.

    MiInitializePool (
        pool, // pool
        node, // node
    )

    pool^.GetMemory = &MiGetPagedMemory
    pool^.ReturnMemory = &MiFreePagedMemory
END

#SECTION "INITtext"
FN MiInitializePools (
    IN node : ^MiNode,
)

    // Initialize the system pools as part of stage 1 Mm init.

    MiInitializeNonpagedPool (
        &node^.PoolRecords[MM_NONPAGED_POOL], // pool
        node, // node
    )

    MiInitializeNonpagedPool (
        &node^.PoolRecords[MM_PRIVILEGED_POOL], // pool
        node, // node
    )

    // Allow privileged pool to use privileged memory.

    node^.PoolRecords[MM_PRIVILEGED_POOL].Flags |=
        MI_POOL_FLAG_USE_PRIVILEGED_MEM

    // Set paged pool to point to nonpaged pool for now, since we don't have
    // enough initialized to deal with faults on paged pool yet.

    node^.PoolTable[MM_NONPAGED_POOL] = &node^.PoolRecords[MM_NONPAGED_POOL]
    node^.PoolTable[MM_PAGED_POOL] = &node^.PoolRecords[MM_NONPAGED_POOL]
    node^.PoolTable[MM_PRIVILEGED_POOL] = &node^.PoolRecords[MM_PRIVILEGED_POOL]

    // Initialize pool cache.

    MiInitializePoolCache ( node )

    // Initialize system space node allocation.

    MiInitializeSpaceNodeAllocation ( node )
END

#SECTION "INITtext"
FN MiInitializePagedPools (
    IN node : ^MiNode,
)

    // Initialize the paged pools as part of stage 2 Mm init.

    MiInitializePagedPool (
        &node^.PoolRecords[MM_PAGED_POOL], // pool
        node, // node
    )

    MiInitializePagedPool (
        &node^.PoolRecords[MM_PAGE_TRACKING_POOL], // pool
        node, // node
    )

    node^.PoolTable[MM_PAGED_POOL] = &node^.PoolRecords[MM_PAGED_POOL]

    node^.PoolTable[MM_PAGE_TRACKING_POOL] =
        &node^.PoolRecords[MM_PAGE_TRACKING_POOL]
END

MiEightBitLog : UBYTE[256] = {
    0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,
    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
}

#MACRO MiAcquirePool ( pool ) [
    KeAcquireApcLockExclusive ( &pool^.Lock )
]

#MACRO MiReleasePool ( pool, ipl ) [
    KeReleaseApcLock ( &pool^.Lock, ipl )
]

FN MiAllocateFromPool (
    IN pool : ^MiPool,
    IN bytes : UWORD,
    IN tag : ULONG,
    IN wait : UWORD,
) : ^VOID

    // Allocate a block of memory of at least the given number of bytes from the
    // specified pool.

    bytes += SIZEOF MiBlock

    bytes += MI_BLOCK_SIZE_MASK
    bytes >>= MI_BLOCK_SIZE_SHIFT

    KeAssert ( bytes <= MI_BLOCK_MAX_REQUEST_UNITS )

    // Bytes now stores size units rather than real bytes.

    // We want to start searching in the bucket ceil(log_2(units)).
    // We place blocks in bucket floor(log_2(units)).

    insertindex := MiEightBitLog[bytes]

    bucketindex := insertindex

    IF 1 << insertindex != bytes THEN
        // Not a power of two number of units, so ceil log is floor log + 1.

        bucketindex += 1
    END

@Retry

    // Do some work outside the pool lock.

    listhead := &pool^.SizeListHeads[bytes]
    i := bytes
    max := 1 << bucketindex

    block : ^MiFreeBlock

    // Lock the pool.

    ipl := MiAcquirePool ( pool )

    // Search the inbetween lists up to the next power of two.

    WHILE i < max DO
        IF NOT RtlEmptyList ( listhead ) THEN
            // Found a block.

            block = CONTAINEROF listhead^.Next TO MiFreeBlock.SizeEntry

            KeAssert ( block^.Block.Size == i )

            GOTO FoundBlock
        END

        i += 1
        listhead += SIZEOF RtlListEntry
    END

    // Search the log lists.

    listhead = &pool^.LogListHeads[bucketindex]
    i = bucketindex

    WHILE i < MI_BLOCK_LOG_BUCKETS DO
        IF NOT RtlEmptyList ( listhead ) THEN
            // Found a block.

            block = CONTAINEROF listhead^.Next TO MiFreeBlock.LogEntry

            KeAssert ( block^.Block.BucketIndex == i )

            GOTO FoundBlock
        END

        i += 1
        listhead += SIZEOF RtlListEntry
    END

    // No sufficient block. We need to allocate a new pool extension.

    ext := MiAllocateExtensionSpace ( pool^.Node )

    IF NOT pool^.GetMemory (
        pool^.Partition, // partition
        ext, // ptr
        1, // pages
        pool^.Flags & MI_POOL_FLAG_USE_PRIVILEGED_MEM, // low
        FALSE, // wait
    ) THEN

        // Failed to allocate a pool extension.

        MiFreeExtensionSpace (
            pool^.Node, // node
            ext, // extension
            FALSE, // flush
        )

        MiReleasePool ( pool, ipl )

        IF NOT wait THEN
            // Caller requested that we don't wait upon failure.

            RETURN NULLPTR
        END

        // Wait for free memory and retry.

        MiWaitForPages (
            pool^.Partition, // partition
            pool^.Flags & MI_POOL_FLAG_USE_PRIVILEGED_MEM, // low
        )

        GOTO Retry
    END

    // Increment external usage. Note that a pool chunk is one page frame.

    pool^.BytesUsedExternally += RTL_PAGE_SIZE

    // Increment pool statistics.

    pool^.BytesUsedInternally += bytes << MI_BLOCK_SIZE_SHIFT

    IF pool^.BytesUsedInternally > pool^.BytesUsedPeak THEN
        pool^.BytesUsedPeak = pool^.BytesUsedInternally
    END

    // Initialize the extension header.

    ext^.Pool = pool
    ext^.References = 1

    // We need to initialize the extension as a single block of the maximum size
    // minus whatever is used by the extension header, minus whatever we need
    // for our allocation.

    // Here we assume that a maximum size block is <= the chunk size, i.e.,
    // there's always room for at least one within a new extension.

    // We also assume that the extension header fits within a single block unit
    // (currently 32 bytes).

    block = CAST ext + MI_BLOCK_MIN_SIZE TO ^MiFreeBlock

    // Initialize the block we'll be returning to the caller.

    block^.Block.Magic = MI_BLOCK_SWIFT_MAGIC
    block^.Block.Size = bytes
    block^.Block.BucketIndex = insertindex
    block^.Block.LastSize = 0
    block^.Block.Tag = tag

    // Initialize the new block.

    newblock := block + (bytes << MI_BLOCK_SIZE_SHIFT)

    newblocksize := (1 << MI_BLOCK_MAX_UNITS_SHIFT) - 1

    KeAssert ( bytes <= newblocksize )

    newblocksize -= bytes

    newbucketindex := MiEightBitLog[newblocksize]

    newblock^.Block.Magic = MI_BLOCK_FREE_MAGIC
    newblock^.Block.Size = newblocksize
    newblock^.Block.BucketIndex = newbucketindex
    newblock^.Block.LastSize = bytes

    // Insert the new block in the free list.

    RtlInsertAtHeadList (
        &pool^.LogListHeads[newbucketindex], // head
        &newblock^.LogEntry, // entry
    )

    RtlInsertAtHeadList (
        &pool^.SizeListHeads[newblocksize], // head
        &newblock^.SizeEntry, // entry
    )

    newblock += newblocksize << MI_BLOCK_SIZE_SHIFT

    i = 0

    WHILE i < MI_FULL_BLOCKS_PER_EXTENSION DO
        // Initialize this full block.

        newblock^.Block.Magic = MI_BLOCK_FREE_MAGIC
        newblock^.Block.Size = 1 << MI_BLOCK_MAX_UNITS_SHIFT
        newblock^.Block.BucketIndex = MI_BLOCK_MAX_UNITS_SHIFT
        newblock^.Block.LastSize = 0

        // Insert the new block into the buckets indexed by floor-log and size.

        RtlInsertAtHeadList (
            &pool^.LogListHeads[MI_BLOCK_MAX_UNITS_SHIFT], // head
            &newblock^.LogEntry, // entry
        )

        RtlInsertAtHeadList (
            &pool^.SizeListHeads[1 << MI_BLOCK_MAX_UNITS_SHIFT], // head
            &newblock^.SizeEntry,
        )

        newblock += MI_BLOCK_MAX_SIZE
        i += 1
    END

    // Unlock the pool.

    MiReleasePool ( pool, ipl )

    // Return the block.

    RETURN block + SIZEOF MiBlock

@FoundBlock

    KeAssert ( block^.Block.Magic == MI_BLOCK_FREE_MAGIC )

    // This block is either perfectly sized, or at least big enough to
    // satisfy the allocation request. In either case, we want to
    // unlink it from the bucket's free list now.

    RtlRemoveEntryList ( &block^.SizeEntry )

    RtlRemoveEntryList ( &block^.LogEntry )

    // Increment the reference count on the pool extension to which this
    // free block belongs, since in any case, we're going to be adding
    // a new allocated block to it.

    ext = MiFindPoolExtension ( block )
    ext^.References += 1

    // Set magic to indicate allocated, since in either case we'll be
    // returning this block to the caller.

    block^.Block.Magic = MI_BLOCK_SWIFT_MAGIC

    // Increment pool statistics.

    pool^.BytesUsedInternally += bytes << MI_BLOCK_SIZE_SHIFT

    IF pool^.BytesUsedInternally > pool^.BytesUsedPeak THEN
        pool^.BytesUsedPeak = pool^.BytesUsedInternally
    END

    // Set tag.

    block^.Block.Tag = tag

    // Load the block size and check.

    newblocksize = block^.Block.Size

    IF newblocksize != bytes THEN
        // The block is too big and must be split.

        KeAssert ( newblocksize >= bytes )

        // Set new bucketindex and size in old block header.

        block^.Block.BucketIndex = insertindex
        block^.Block.Size = bytes

        // Calculate the size of the new block.

        newblocksize -= bytes

        // Calculate the index of the bucket to place the new block in.

        newbucketindex = MiEightBitLog[newblocksize]

        // Get a pointer to the new block.

        newblock = block + (bytes << MI_BLOCK_SIZE_SHIFT)

        // Create new block header.

        newblock^.Block.Size = newblocksize
        newblock^.Block.LastSize = bytes
        newblock^.Block.BucketIndex = newbucketindex
        newblock^.Block.Magic = MI_BLOCK_FREE_MAGIC

        // Insert the new block into the buckets indexed by floor-log and size.

        RtlInsertAtHeadList (
            &pool^.LogListHeads[newbucketindex], // head
            &newblock^.LogEntry, // entry
        )

        RtlInsertAtHeadList (
            &pool^.SizeListHeads[newblocksize], // head
            &newblock^.SizeEntry,
        )

        // Update next block to point to the new block, unless it is
        // aligned to a maximum block size, in which case there is no next
        // block.

        newblock += newblocksize << MI_BLOCK_SIZE_SHIFT

        IF newblock & MI_BLOCK_MAX_SIZE_MASK THEN
            newblock^.Block.LastSize = newblocksize
        END
    END

    // Unlock the pool.

    MiReleasePool ( pool, ipl )

    // Return the block.

    RETURN block + SIZEOF MiBlock
END

FN MiFreeToPool (
    IN ptr : ^VOID,
    IN tag : ULONG,
)

    // Free the pool block. We're only passed blocks that were small enough to
    // go in POOLSPACE.

    otherblock : ^MiFreeBlock

    // Acquire a pointer to the block.

    block := CAST ptr - SIZEOF MiBlock TO ^MiFreeBlock

    IF tag != block^.Block.Tag THEN
        KeCrash ( "MiFreeToPool: wrong tag %x on %p (expected %x)\n",
            block^.Block.Tag, ptr, tag )
    END

    // Acquire a pointer to the pool extension.

    ext := MiFindPoolExtension ( block )

    pool := ext^.Pool

    merged := FALSE

    blocksize := block^.Block.Size

    // Lock the pool in which the block resides.

    ipl := MiAcquirePool ( pool )

    pool^.BytesUsedInternally -= blocksize << MI_BLOCK_SIZE_SHIFT

    IF block^.Block.LastSize THEN
        // Check the block to the left for merging.

        otherblock = block - (block^.Block.LastSize << MI_BLOCK_SIZE_SHIFT)

        KeAssert ( otherblock^.Block.Size == block^.Block.LastSize )

        IF otherblock^.Block.Magic == MI_BLOCK_FREE_MAGIC THEN
            // Free! Merge left.

            merged = TRUE

            // Remove it from old free lists.

            RtlRemoveEntryList ( &otherblock^.LogEntry )

            RtlRemoveEntryList ( &otherblock^.SizeEntry )

            // Increment our block size.

            blocksize += otherblock^.Block.Size

#IF BLD_CHK
            // Invalidate magic number of old block, since it's now in the
            // middle of this new block.

            block^.Block.Magic = MI_BLOCK_INVALID_MAGIC
#END

            // Set block pointer to left block.

            block = otherblock
        END
    END

    // Get a pointer to the block to the right.

    otherblock = block + (blocksize << MI_BLOCK_SIZE_SHIFT)

    IF otherblock & MI_BLOCK_MAX_SIZE_MASK THEN
        // A block to the right exists if it's not aligned to the maximum block
        // size.

        IF otherblock^.Block.Magic == MI_BLOCK_FREE_MAGIC THEN
            // Free! Merge right.

            merged = TRUE

            // Remove it from old free lists.

            RtlRemoveEntryList ( &otherblock^.LogEntry )

            RtlRemoveEntryList ( &otherblock^.SizeEntry )

            // Increment our block size.

            blocksize += otherblock^.Block.Size

#IF BLD_CHK
            // Invalidate magic number of old block, since it's now in the
            // middle of the new block.

            otherblock^.Block.Magic = MI_BLOCK_INVALID_MAGIC
#END
        END
    END

    // Decrement the refcount on the extension.

    ext^.References -= 1

    IF NOT ext^.References THEN
        // The extension is totally empty and should be released.
        // This involves removing all of its blocks (except this one!) from the
        // free lists and then returning the extension to whence it came.

        // The form of the extension should be a partial block (with a piece cut
        // out for the extension header) plus a number of maximum sized blocks.
        // First unlink the partial block.

        otherblock = CAST ext + MI_BLOCK_MIN_SIZE TO ^MiFreeBlock

        IF otherblock != block THEN
            KeAssert ( otherblock^.Block.Magic == MI_BLOCK_FREE_MAGIC )

            RtlRemoveEntryList ( &otherblock^.LogEntry )

            RtlRemoveEntryList ( &otherblock^.SizeEntry )
        END

        // Now unlink the full blocks.

        otherblock += MI_BLOCK_MAX_SIZE - MI_BLOCK_MIN_SIZE

        i := 0

        WHILE i < MI_FULL_BLOCKS_PER_EXTENSION DO
            IF otherblock != block THEN
                KeAssert ( otherblock^.Block.Magic == MI_BLOCK_FREE_MAGIC )

                RtlRemoveEntryList ( &otherblock^.LogEntry )

                RtlRemoveEntryList ( &otherblock^.SizeEntry )
            END

            i += 1
            otherblock += MI_BLOCK_MAX_SIZE
        END

        pool^.BytesUsedExternally -= RTL_PAGE_SIZE

        // Return the extension.

        pool^.ReturnMemory (
            pool^.Partition, // partition
            ext, // ptr
            1, // pages
        )

        MiFreeExtensionSpace (
            pool^.Node, // node
            ext, // extension
            TRUE, // flush
        )

        // Unlock pool.

        MiReleasePool ( pool, ipl )

        LEAVE
    END

    IF merged THEN
        // Re-calculate bucket index and set new fields.

        block^.Block.BucketIndex = MiEightBitLog[blocksize]
        block^.Block.Size = blocksize

        // Set last size of block to our right.

        otherblock = block + (blocksize << MI_BLOCK_SIZE_SHIFT)

        IF otherblock & MI_BLOCK_MAX_SIZE_MASK THEN
            otherblock^.Block.LastSize = blocksize
        END
    END

    // Set free magic.

    block^.Block.Magic = MI_BLOCK_FREE_MAGIC

    // Put on free lists.

    RtlInsertAtHeadList (
        &pool^.LogListHeads[block^.Block.BucketIndex], // head
        &block^.LogEntry, // entry
    )

    RtlInsertAtHeadList (
        &pool^.SizeListHeads[blocksize], // head
        &block^.SizeEntry, // entry
    )

    // Unlock pool and return.

    MiReleasePool ( pool, ipl )
END

FN MiAllocatePageAligned (
    IN pool : ^MiPool,
    IN bytes : UWORD,
    IN tag : ULONG,
    IN wait : UWORD,
) : ^VOID

    // Perform a page-aligned allocation from the specified pool.

    // Turn bytes into a rounded up count of pages.

    bytes = ((bytes + RTL_PAGE_SIZE - 1) & ~(RTL_PAGE_SIZE - 1)) >>
        RTL_PAGE_SHIFT

    // First acquire some room in dynamic space.

    spacenode := MiAllocateDynamicSpace (
        pool^.Node, // node
        bytes, // pages
        wait, // wait
    )

    IF NOT spacenode THEN
        RETURN NULLPTR
    END

    // Initialize the node as a page aligned pool node.

    spacenode^.U.Pool.Pool = pool
    spacenode^.U.Pool.Tag = tag

    // Call the pool callback to initialize the allocation.

    IF NOT pool^.GetMemory (
        pool^.Partition, // partition
        spacenode^.StartVa, // ptr
        bytes, // pages
        pool^.Flags & MI_POOL_FLAG_USE_PRIVILEGED_MEM, // low
        wait, // wait
    ) THEN
        // Caller told us not to wait and we didn't get the page frames we
        // wanted, so just return.

        // No need to flush the dynamic space since we didn't map anything
        // into it.

        MiReleaseDynamicSpace (
            pool^.Node, // node
            spacenode, // spacenode
            FALSE, // flush
        )

        RETURN NULLPTR
    END

    RETURN spacenode^.StartVa
END

FN MiFreePageAligned (
    IN ptr : ^VOID,
    IN tag : ULONG,
)

    // Free a page-aligned allocation to pool.

    // Look up the dynamic space node.

    spacenode := MiFindDynamicSpaceNode ( ptr )

    KeAssert ( spacenode != NULLPTR )
    KeAssert ( spacenode^.StartVa == ptr )

    IF spacenode^.U.Pool.Tag != tag THEN
        KeCrash ( "MiFreePageAligned: wrong tag %x on %p, expected %x\n",
            spacenode^.StartVa, spacenode^.U.Pool.Tag, tag )
    END

    // Uninitialize the allocation.

#IF BLD_CHK
    // Fill with easily recognizable garbage to crash anybody who improperly
    // uses this memory.

    RtlFillMemoryWithUlong (
        ptr, // ptr
        spacenode^.EndVa - spacenode^.StartVa, // sz
        0xFAAAAAAA, // ulong
    )
#END

    pool := spacenode^.U.Pool.Pool

    pool^.ReturnMemory (
        pool^.Partition, // partition
        ptr, // ptr
        (spacenode^.EndVa - spacenode^.StartVa) >> RTL_PAGE_SHIFT, // pages
    )

    // Free the dynamic space.

    MiReleaseDynamicSpace (
        pool^.Node, // node
        spacenode, // spacenode
        TRUE, // flush
    )
END

EXPORT FN MmAllocatePool (
    IN poolindex : UWORD,
    IN bytes : UWORD,
    IN tag : ULONG,
    IN wait : UWORD,
) : ^VOID

    // Allocate a block of memory from the specified pool.

    KeAssert ( bytes != 0 )

    thread : ^PsThread = NULLPTR
    savewait := wait

    IF poolindex == MM_NONPAGED_POOL THEN
        thread = PsCurrentThread ()

        // If this is a VM-privileged thread allocating nonpaged pool, override
        // the wait parameter to FALSE, so that we don't deadlock waiting for
        // page frames.

        IF thread^.VmPrivileged THEN
            wait = FALSE
        ELSE
            thread = NULLPTR
        END
    END

@Retry

    pool := MiBootNode^.PoolTable[poolindex]

    ptr : ^VOID

    IF bytes < MI_BLOCK_MAX_REQUEST_SIZE THEN
        // Allocate from the small block pool.

        ptr = MiAllocateFromPool (
            pool, // pool
            bytes, // bytes
            tag, // tag
            wait, // wait
        )

    ELSE
        // Allocate from page aligned pool.

        ptr = MiAllocatePageAligned (
            pool, // pool
            bytes, // bytes
            tag, // tag
            wait, // wait
        )
    END

    IF NOT ptr AND thread THEN
        // This was a VM-privileged thread allocating nonpaged pool, and it
        // just failed to get a block from normal pool. Try again from
        // privileged pool (which will take from the reserved final page
        // frames if necessary).

        poolindex = MM_PRIVILEGED_POOL
        thread = NULLPTR
        wait = savewait

        GOTO Retry
    END

#IF BLD_CHK
    IF NOT ptr THEN
        RETURN NULLPTR
    END

    // Make sure it gets initialized by filling it with garbage.
    // Garbage is chosen to be easily recognizable on a hex dump.

    RtlFillMemoryWithUlong (
        ptr, // ptr
        bytes, // sz
        0xFBCDEFAF, // ulong
    )
#END

    RETURN ptr
END

EXPORT FN MmFreePool (
    IN ptr : ^VOID,
    IN tag : ULONG,
)

    // Free the block of pool.

    IF ptr >= MI_POOL_SPACE AND ptr < MI_POOL_SPACE_END THEN
        // This block resides in small pool.

        MiFreeToPool (
            ptr, // ptr
            tag, // tag
        )

        LEAVE
    END

    // Page aligned pool.

    MiFreePageAligned (
        ptr, // ptr
        tag, // tag
    )
END

#SECTION "PAGEtext"
FN MmGetOverheadOfBlock (
    IN ptr : ^VOID,
) : UWORD

    // Return the total bytes overhead consumed by a pool allocation.

    IF ptr >= MI_POOL_SPACE AND ptr < MI_POOL_SPACE_END THEN
        // This block is in small pool.

        block := CAST ptr - SIZEOF MiBlock TO ^MiBlock

        RETURN block^.Size << MI_BLOCK_SIZE_SHIFT
    END

    // Page-aligned.

    node := MiFindDynamicSpaceNode ( ptr )

    KeAssert ( node != NULLPTR )
    KeAssert ( node^.StartVa == ptr )

    RETURN node^.EndVa - node^.StartVa
END

#SECTION "PAGEtext"
FN MmGetOverheadOfBytes (
    IN bytes : UWORD,
) : UWORD

    // Calculate the total overhead in bytes that the pool allocation for the
    // given number of bytes would consume. This *MUST* match what is returned
    // by MmGetChargeFromBlock for a pool allocation that really was for this
    // many bytes.

    IF bytes == 0 THEN
        RETURN 0
    END

    IF bytes >= MI_BLOCK_MAX_REQUEST_SIZE THEN
        // Page-aligned charge.

        RETURN (bytes + RTL_PAGE_OFFSET_MASK) & RTL_PAGE_NUMBER_MASK
    END

    // Small block charge.

    RETURN (bytes + SIZEOF MiBlock + MI_BLOCK_SIZE_MASK) & ~MI_BLOCK_SIZE_MASK
END