//
// Implements support for memory nodes.
//

// Partial NUMA support is implemented throughout the memory manager, mostly to
// make it easier to add full NUMA support later. Some things that would need
// doing in order to make it really great:
//
//  o Splitting the worker thread pool to be per-node.
//  o Splitting the object and thread reaper lists to be per-node.
//  o Splitting the object allocation caches (possibly by remodeling ObuType to
//    be itself a per-node thing).
//  o Splitting the IO packet allocation caches and other caches.
//  o NUMA awareness in the scheduler (just enough to isolate scheduling
//    decisions to processors on the local node).

#INCLUDE "Mmp.hjk"

#SECTION "INITtext"
FN MmpInitializeAvailablePageList (
    IN list : ^MmpAvailablePageList,
)

    // Initialize a list of available pages.

    list^.Count = 0

    i := 0

    WHILE i < MMP_COLOR_COUNT DO
        RtlInitializeList ( &list^.Heads[i] )

        i += 1
    END
END

#SECTION "INITtext"
FN MmpInitializeNodeStage1 (
    IN mmnode : ^MmpNode,
)

    // Perform stage 1 (physical) initialization for a memory node.

    // Initialize the locks.

    KeInitializeLock ( &mmnode^.ListLock )

    KeInitializeLock ( &mmnode^.CommitLock )

    // Initialize the events.

    KeInitializeEvent (
        &mmnode^.LowMemoryEvent, // event
        "LowMemory", // name
        FALSE, // notification
        FALSE, // signalstate
    )

    KeInitializeEvent (
        &mmnode^.PageAvailableEvent, // event
        "PageAvailable", // name
        TRUE, // notification
        TRUE, // signalstate
    )

    KeInitializeEvent (
        &mmnode^.LowPageAvailableEvent, // event
        "LowPageAvailable", // name
        TRUE, // notification
        TRUE, // signalstate
    )

    KeInitializeEvent (
        &mmnode^.ModifiedPageEvent, // event
        "ModifiedPages", // name
        FALSE, // notification
        FALSE, // signalstate
    )

    KeInitializeEvent (
        &mmnode^.ZeroPageEvent, // event
        "ZeroPages", // name
        FALSE, // notification
        FALSE, // signalstate
    )

    // Initialize the list heads.

    RtlInitializeList ( &mmnode^.ModifiedListHead )
    mmnode^.ModifiedPageCount = 0

    MmpInitializeAvailablePageList ( &mmnode^.FreeList )
    MmpInitializeAvailablePageList ( &mmnode^.ZeroList )
    MmpInitializeAvailablePageList ( &mmnode^.StandbyList )

    mmnode^.AvailablePageCount = 0
    mmnode^.FluidPageCount = 0

    // Initialize commit.

    mmnode^.CommitUsage = 0
    mmnode^.CommitLimit = 0
    mmnode^.TheoreticalCommitLimit = 0

    // Initialize thresholds.

    mmnode^.LowPageCount = 0
    mmnode^.SufficientPageCount = 0
    mmnode^.ModifiedPageMaximum = 0
    mmnode^.ZeroingThreshold = 0

    mmnode^.SizeLevel = MM_UNINITIALIZED_SYSTEM
END

#SECTION "INITtext"
FN MmpSetNodeSize (
    IN mmnode : ^MmpNode,
)

    // Calculate the node size and various threshold values.

    membytes := mmnode^.TotalPages << RTL_PAGE_SHIFT
    size : UWORD

    IF membytes <= 3 * 1024 * 1024 THEN
        // Tiny, 0-3MB.

        size = MM_TINY_SYSTEM

    ELSEIF membytes <= 5 * 1024 * 1024 THEN
        // Small, 3-5MB.

        size = MM_SMALL_SYSTEM

    ELSEIF membytes <= 9 * 1024 * 1024 THEN
        // Medium, 5-9MB.

        size = MM_MEDIUM_SYSTEM

    ELSEIF membytes <= 33 * 1024 * 1024 THEN
        // Large, 9-33MB.

        size = MM_LARGE_SYSTEM

    ELSE
        // Massive, >33MB.

        size = MM_MASSIVE_SYSTEM
    END

    mmnode^.SizeLevel = size

    // Set the modified page maximum.

    IF size <= MM_TINY_SYSTEM THEN
        mmnode^.ModifiedPageMaximum = 50

    ELSEIF size <= MM_LARGE_SYSTEM THEN
        mmnode^.ModifiedPageMaximum = 100

    ELSE
        mmnode^.ModifiedPageMaximum = 300
    END

    // Set the page zeroing threshold.

    IF size <= MM_TINY_SYSTEM THEN
        mmnode^.ZeroingThreshold = 75

    ELSEIF size <= MM_MEDIUM_SYSTEM THEN
        mmnode^.ZeroingThreshold = 150

    ELSE
        mmnode^.ZeroingThreshold = 500
    END

    // Set the paging thresholds.

    IF size <= MM_TINY_SYSTEM THEN
        mmnode^.LowPageCount = MMP_BLOCK_FOR_PAGES_THRESHOLD + 8
        mmnode^.SufficientPageCount = MMP_BLOCK_FOR_PAGES_THRESHOLD + 50

    ELSEIF size <= MM_LARGE_SYSTEM THEN
        mmnode^.LowPageCount = MMP_BLOCK_FOR_PAGES_THRESHOLD + 20
        mmnode^.SufficientPageCount = MMP_BLOCK_FOR_PAGES_THRESHOLD + 100

    ELSE
        mmnode^.LowPageCount = MMP_BLOCK_FOR_PAGES_THRESHOLD + 100
        mmnode^.SufficientPageCount = MMP_BLOCK_FOR_PAGES_THRESHOLD + 500
    END
END

FN MmpInsertPageIntoList (
    IN list : ^MmpAvailablePageList,
    IN pfe : ^MmpPfe,
    IN head : UWORD,
)

    // Insert the page into the specified list, into the correct color bucket.

    pfn := MmpPfeToPfn ( pfe )

    list^.Count += 1

    IF head THEN
        RtlInsertAtHeadList (
            &list^.Heads[MmpPfnColor ( pfn )], // head
            &pfe^.Entry, // entry
        )

    ELSE
        RtlInsertAtTailList (
            &list^.Heads[MmpPfnColor ( pfn )], // head
            &pfe^.Entry, // entry
        )
    END
END

FN MmpAllocatePageFromList (
    IN list : ^MmpAvailablePageList,
    IN color : UWORD,
) : ^MmpPfe

    // Allocate a page, preferably with the given color, from the list.
    // The caller already ensured there's a page to remove from the list.

    list^.Count -= 1

#IF ( == MMP_COLOR_COUNT 1 )
    KeAssert ( NOT RtlEmptyList ( &list^.Heads[0] ) )

    entry := list^.Heads[0].Next

    RtlRemoveEntryList ( entry )

    RETURN CONTAINEROF entry TO MmpPfe.Entry

#ELSE
    i := 0

    WHILE i < MMP_COLOR_COUNT DO
        entry := list^.Heads[color].Next

        IF entry != &list^.Heads[color] THEN
            // Found one. Pop it from the head.

            RtlRemoveEntryList ( entry )

            RETURN CONTAINEROF entry TO MmpPfe.Entry
        END

        color = (color + 1) & (MMP_COLOR_COUNT - 1)
        i += 1
    END

    // Shouldn't be reachable - caller saw there were items in this list.

    KeAssert ( FALSE )
#END

END

FN MmpAllocatePageFromStandbyList (
    IN mmnode : ^MmpNode,
    IN color : UWORD,
) : ^MmpPfe

    // Allocate a page from the given standby list.

    pfe := MmpAllocatePageFromList (
        &mmnode^.StandbyList, // list
        color, // color
    )

    // Disassociate it from its object.
    // If this is paged pool, paged executive, or a page table, this is a "dummy
    // object" used only for locking purposes.

    object := pfe^.Object

    // No need to call this as APC-safe since we're already blocking out APCs
    // by holding the list lock.

    KeAcquireLockExclusive ( &object^.StructureLock )

    // Set object pointer to NULLPTR. This is important so that it notices that
    // the owner of the page "changed" if there is a concurrent reference being
    // taken out on this page.

    pfe^.Object = NULLPTR

    IF pfe^.Type == MMP_BACKED_PFE_TYPE THEN
        // Remove from AVL tree of object.

        ob := CONTAINEROF object TO MmuBackedObject.Hdr

        RtlRemoveAvl (
            &ob^.PageTreeRoot, // root
            &pfe^.U.Backed.Entry, // node
        )

    ELSE

        KeAssert ( pfe^.Type == MMP_ANON_PFE_TYPE )

        // Reset the tracking table entry for this anonymous page.

        entry := pfe^.U.Anon.TrackingTableEntry
        entry^ = pfe^.U.Anon.Backing

        // If this wasn't a page of paged pool or the paged executive, the
        // tracking table entry resides in page tracking pool, and we can
        // decrement the refcount on that page of tracking pool, which may
        // allow it to be paged out if it gets trimmed from the system working
        // set.
        //
        // Note that if it WAS paged pool or paged executive, the containing
        // page for the tracking PTE was an actual system space page table -
        // we never page those out!
        //
        // Note that *user* page tables can be reclaimed, but are *not*
        // reclaimed here. Freeing them up when the last PTE has been trimmed
        // out of a page table is the province of architecture-specific pmap
        // code.

        IF pfe^.Flags & MMP_PAGED_POOL_PFE_FLAG == 0 THEN
            poolpfe := MmpPfnToPfe (
                MmpPfnFromPte (
                    (MmpPteAddress ( entry ))^
                )
            )

            // Tracking pool should consist of anonymous pages.

            KeAssert ( poolpfe^.Type == MMP_ANON_PFE_TYPE )

            // This page of tracking pool should have at least 2
            // references: one for being pinned (due to having this tracking
            // entry resident), and another for being in the system working
            // set.

            KeAssert ( poolpfe^.References >= 2 )

            poolpfe^.References -= 1
        END
    END

    KeReleaseLock ( &object^.StructureLock )

    RETURN pfe
END

FN MmpAllocatePage (
    IN mmnode : ^MmpNode,
    IN zeroed : UWORD,
    IN low : UWORD,
    IN color : UWORD,
) : ^MmpPfe

    // Allocate a page from the given node. If none available, return
    // NULLPTR.

    pfe : ^MmpPfe = NULLPTR
    mustzero := FALSE
    listentry : ^RtlListEntry

    // Ensure color within range.

    color &= MMP_COLOR_COUNT - 1

    ipl := MmpAcquireListExclusive ( mmnode )

    avail := mmnode^.AvailablePageCount

    IF avail == 0 THEN
        // No pages.

        GOTO Out
    END

    IF NOT low AND avail <= MMP_BLOCK_FOR_PAGES_THRESHOLD THEN
        // Not enough pages.

        GOTO Out
    END

    IF zeroed THEN
        IF mmnode^.ZeroList.Count THEN
            pfe = MmpAllocatePageFromList (
                &mmnode^.ZeroList, // list
                color, // color
            )

        ELSE
            // Need to grab a free or standby page. Make sure to zero out the
            // page outside the list lock.

            mustzero = TRUE

            IF mmnode^.FreeList.Count THEN
                pfe = MmpAllocatePageFromList (
                    &mmnode^.FreeList, // list
                    color, // color
                )

            ELSE
                pfe = MmpAllocatePageFromStandbyList (
                    mmnode, // mmnode
                    color, // color
                )
            END
        END

    ELSEIF mmnode^.FreeList.Count THEN
        pfe = MmpAllocatePageFromList (
            &mmnode^.FreeList, // list
            color, // color
        )

    ELSEIF mmnode^.ZeroList.Count THEN
        pfe = MmpAllocatePageFromList (
            &mmnode^.ZeroList, // list
            color, // color
        )

    ELSE
        pfe = MmpAllocatePageFromStandbyList (
            mmnode, // mmnode
            color, // color
        )
    END

    avail -= 1
    mmnode^.AvailablePageCount = avail

    IF avail < mmnode^.LowPageCount THEN
        // Too few pages. Signal the low memory event.

        KeSignalEvent (
            &mmnode^.LowMemoryEvent, // event
            0, // priorityboost
        )
    END

@Out

    MmpReleaseList ( mmnode, ipl )

    IF mustzero THEN
        KeAssert ( pfe != NULLPTR )

        MmZeroPage ( MmpPfeToPfn ( pfe ) )
    END

    RETURN pfe
END

FN MmpFreePage (
    IN mmnode : ^MmpNode,
    IN pfe : ^MmpPfe,
)

    // Free the page to the specified node.

    ipl := MmpAcquireListExclusive ( mmnode )

    avail := mmnode^.AvailablePageCount
    avail += 1

    mmnode^.AvailablePageCount = avail

    // Insert at the head so that we reuse it more quickly (hot in cache).

    MmpInsertPageIntoList (
        &mmnode^.FreeList, // list
        pfe, // pfe
        TRUE, // head
    )

    IF mmnode^.FreeList.Count > mmnode^.ZeroingThreshold AND
        mmnode^.ZeroPageEvent.Header.SignalCount == 0 THEN

        // Wake the zero page worker.

        KeSignalEvent (
            &mmnode^.ZeroPageEvent, // event
            0, // priorityboost
        )
    END

    MmpReleaseList ( mmnode, ipl )

    // Wake waiters for available pages.

    MmpNewAvailablePage ( mmnode, avail )
END

#DEFINE MMP_MIN_LOOPS_FOR_BACKOFF 2
#DEFINE MMP_INITIAL_BACKOFF_MS 5
#DEFINE MMP_MAX_BACKOFF_MS 1000

FN MmpWaitForPages (
    IN mmnode : ^MmpNode,
    IN low : UWORD,
) : UWORD

    // Wait for available pages to exist from the given node.
    // This function provides no guarantees - caller will have to repeatedly
    // call it until MmpAllocatePage yields a page.

    // Randomized exponential back-off is performed to mitigate thundering herd.
    // Some bits of the current thread pointer are used as a seed for xorshift.

    loops := 0
    threshold : UWORD
    event : ^KeEvent

    timeout : RtlUquad
    RtlSetUquadToUlong ( &timeout, 60000 )

    backoff : RtlUquad
    backoffms := MMP_INITIAL_BACKOFF_MS

    IF low THEN
        threshold = 0
        event = &mmnode^.LowPageAvailableEvent

    ELSE
        threshold = MMP_BLOCK_FOR_PAGES_THRESHOLD
        event = &mmnode^.PageAvailableEvent
    END

    ipl := MmpAcquireListExclusive ( mmnode )

    WHILE mmnode^.AvailablePageCount <= threshold DO
        // Wait on the event until we catch the available page count at a
        // sufficient level.

        KeResetEvent ( event )

        MmpReleaseList ( mmnode, ipl )

        IF loops >= MMP_MIN_LOOPS_FOR_BACKOFF THEN
            // Perform exponential backoff.

            seed : UWORD

            IF loops == MMP_MIN_LOOPS_FOR_BACKOFF THEN
                // Initialize the xorshift seed which is the low bits of the
                // current thread pointer shifted right by the natural
                // alignment and XORed with the VPN and the current uptime.
                // This tries to improve uniqueness of the seed between each
                // thread and over time.

                thrd := KeCurrentThread ()
                seed = (thrd >> RTL_ALIGN_SHIFT) $ (thrd >> RTL_PAGE_SHIFT)
#IF ( == BLD_BITS 32 )
                seed $= KeCurrentVolatileNode ()^.SharedUserPage^.Uptime.Low
#ELSE
                seed $= KeCurrentVolatileNode ()^.SharedUserPage^.Uptime.Quad
#END
            END

            // Jitter the backoff interval by the low 3 bits of the seed.

            backoffms += seed & 7

            IF backoffms >= MMP_MAX_BACKOFF_MS THEN
                // This thread has been waiting a long time for pages and may
                // be starved by newcoming threads. Reset the backoff interval
                // to a small random value in order to compete with them.

                backoffms = 1 + (seed & 31)
                loops = 0
            END

            RtlSetUquadToUlong (
                &backoff, // uquad
                backoffms, // ulong
            )

            KeSleep (
                &backoff, // interval
                KE_KERNEL_MODE, // waitmode
                FALSE, // alertable
            )

            // Multiply the backoff interval by two.

            backoffms *= 2

            seed $= seed << 13
            seed $= seed >> 17
            seed $= seed << 5
        END

        status := KeWaitForSingleObject (
            KE_KERNEL_MODE, // waitmode
            KE_UNALERTABLE, // alertable
            &timeout, // timeout
            &event^.Header, // object
        )

        IF status == OS_STATUS_WAIT_TIMEOUT THEN
            // If a node is deadlocked, this is a fatal
            // condition.

            KeCrash (
                "Mm: Deadlocked. EVI=%u FRE=%u ZRO=%u MOD=%u AVL=%u\n",
                mmnode^.StandbyList.Count,
                mmnode^.FreeList.Count,
                mmnode^.ZeroList.Count,
                mmnode^.ModifiedPageCount,
                mmnode^.AvailablePageCount,
            )
        END

        loops += 1

        ipl = MmpAcquireListExclusive ( mmnode )
    END

    MmpReleaseList ( mmnode, ipl )
END

FN MmpAllocatePageWait (
    IN mmnode : ^MmpNode,
    IN zeroed : UWORD,
    IN low : UWORD,
    IN color : UWORD,
) : ^MmpPfe

    // Allocate a page with the given parameters. Allowed to wait. Returns
    // NULLPTR on timeout (indicative of node deadlock).

    WHILE TRUE DO
        pfe := MmpAllocatePage (
            mmnode, // mmnode
            zeroed, // zeroed
            low, // low
            color, // color
        )

        IF pfe THEN
            RETURN pfe
        END

        MmpWaitForPages (
            mmnode, // mmnode
            low, // low
        )
    END
END

FN MmuChargeCommit (
    IN mmnode : ^MmpNode,
    IN pages : UWORD,
    IN wait : UWORD,
) : OsStatus

    // Attempt to charge the specified commit to the memory node.
    // It would be unwise to page this routine as it can be called during
    // nonpaged pool allocation.

    status := OS_STATUS_SUCCESS

    // Acquire the commit lock to block out pagefile contraction that
    // might lower the commit limit and cause us to proceed erroneously.

    ipl := KeAcquireApcLockExclusive ( &mmnode^.CommitLock )

    IF mmnode^.CommitUsage + pages < pages THEN
        // Overflows the commit usage.

        status = OS_STATUS_COMMIT_EXCEEDED

        GOTO Exit
    END

    IF mmnode^.CommitUsage + pages > mmnode^.TheoreticalCommitLimit THEN
        // Don't even bother trying, there's no way to extend the pagefiles to
        // accommodate this because the theoretical commit limit represents what
        // we could theoretically increase the commit limit to if we were to
        // extend all of the pagefiles to their maximum size.

        status = OS_STATUS_COMMIT_EXCEEDED

        GOTO Exit
    END

    // Increase the commit usage.
    // If it goes over the limit, us and anyone else trying to charge commit
    // will spin below until a pagefile expansion succeeds.

    KeIncrementPtr (
        &mmnode^.CommitUsage, // ptr
        pages, // inc
    )

    WHILE mmnode^.CommitUsage > mmnode^.CommitLimit DO
        KeReleaseApcLock ( &mmnode^.CommitLock, ipl )

        status = MmpExpandPageFiles (
            mmnode, // mmnode
            wait, // wait
            FALSE, // full
        )

        IF NOT wait THEN
            // Proceed no matter what.

            RETURN status
        END

        ipl = KeAcquireApcLockExclusive ( &mmnode^.CommitLock )

        IF OsError ( status ) THEN
            // Failed to increase commit limit.
            // Remove the commit charge.

            KeIncrementPtr (
                &mmnode^.CommitUsage, // ptr
                -pages, // inc
            )

            GOTO Exit
        END
    END

@Exit

    KeReleaseApcLock ( &mmnode^.CommitLock, ipl )

    RETURN status
END

FN MmuUnchargeCommit (
    IN mmnode : ^MmpNode,
    IN pages : UWORD,
)

    // Remove the specified commit charge from the node.
    // If pagefiles can now be contracted, that will be noticed at the next tick
    // of the responsible worker thread.

    old := KeIncrementPtr (
        &mmnode^.CommitUsage, // ptr
        -pages, // inc
    )

    KeAssert ( pages <= old )
END

EXPORT FN MmGetKeNodeSize (
    IN kenode : ^KeuNode,
) : UWORD

    // Get the size of the given node.

    RETURN kenode^.Mmp^.SizeLevel
END

EXPORT FN MmGetMmNodeSize (
    IN mmnode : ^MmpNode,
) : UWORD

    // Get the size of the given node.

    RETURN mmnode^.SizeLevel
END

FN MmuNodeToKeNode (
    IN mmnode : ^MmpNode,
) : ^KeuNode

    RETURN mmnode^.KeNode
END