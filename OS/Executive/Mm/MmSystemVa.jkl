//
// Implements system address space allocation for the MINTIA Memory Manager.
//

#INCLUDE "Mi.hjk"
#INCLUDE "../../Loader/Headers/Loader.hjk"

STRUCT MiChunkEntry
    Next : ^MiChunkEntry,
END

PUBLIC MiPoolSpace : MiChunkSpace
PUBLIC MiCacheSpace : MiChunkSpace

PUBLIC MiCacheSpaceChunkPages : UWORD

#DEFINE MI_DEFERRED_FLUSH_PAIRS 64
#DEFINE MI_DEFERRED_FLUSH_PAGES 128

MiDynamicSpaceRoot : RtlAvlNode
MiDynamicSpaceLock : KeLock
MiDynamicSpaceTreeLock : KeLock
MiDynamicSpaceFreeNodeListHead : ^MiDynamicSpaceFreeNode
MiDynamicSpaceBitmap : RtlBitmapHeader
MiDynamicSpaceHint : UWORD

STRUCT MiDeferredFlush
    Ptr : ^VOID,
    Pages : UWORD,
END

MiDeferredFlushTable : MiDeferredFlush[MI_DEFERRED_FLUSH_PAIRS]
MiDeferredFlushIndex : UWORD
MiDeferredFlushPages : UWORD

#DEFINE MI_FREE_NODE_CHUNK_SIZE 8

STRUCT MiDynamicSpaceFreeNode
    Next : ^MiDynamicSpaceFreeNode,
END

#SECTION "INITtext"
FN MiInitializeChunkSpace (
    IN chunkspace : ^MiChunkSpace,
    IN base : ^VOID,
    IN entryshift : UWORD,
    IN pages : UWORD,
)

    // Initialize a chunked region of system space.

    entrysize := 1 << entryshift
    entrycount := (pages << RTL_PAGE_SHIFT) / entrysize
    entrypages := entrysize >> RTL_PAGE_SHIFT

    chunkspace^.EntryShift = entryshift
    chunkspace^.EntryCount = entrycount
    chunkspace^.FreeListHead = NULLPTR

    chunkspace^.PendingFlushCount = 0

    KeInitializeLock ( &chunkspace^.Lock )

    // Link the chunks through the PTEs.

    pte := MiPteAddress ( base )

    WHILE entrycount DO
        entry := CAST pte TO ^MiChunkEntry

        entry^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = entry

        entrycount -= 1
        pte += entrypages * SIZEOF MiPte
    END
END

FN MiFlushChunkSpace (
    IN chunkspace : ^MiChunkSpace,
)

    // Flush TB entries across all processors for the delayed chunks.

    // Insert these entries into the free list.

    i := 0
    max := chunkspace^.PendingFlushCount

    WHILE i < max DO
        chunk := CAST MiPteAddress ( chunkspace^.PendingFlush[i] )
            TO ^MiChunkEntry

        chunk^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = chunk

        i += 1
    END

    chunkspace^.PendingFlushCount = 0

    // Flush the TB.

    IF chunkspace^.EntryShift == RTL_PAGE_SHIFT AND
        max < 16 THEN

        // Chunks are pages, and there's fewer than 16.
        // Flush them individually.

        KeFlushMultipleTb (
            &chunkspace^.PendingFlush[0], // vaddrtable
            max, // pagecount
        )

    ELSE
        // Chunks are multiple pages, or there's more than 16.
        // Flush everyone's entire TB. Don't keep global pages.

        KeSweepTb ( FALSE )
    END
END

FN MiAllocateChunkSpace (
    IN chunkspace : ^MiChunkSpace,
) : ^VOID

    // Allocate a chunk from the given chunk space.
    // Returns the virtual address of the chunk. Nothing is mapped into the
    // returned virtual address - this is the caller's responsibility.
    // Returns NULLPTR if no chunks remaining.

    ipl := KeAcquireApcLockExclusive ( &chunkspace^.Lock )

    chunk := chunkspace^.FreeListHead

    IF NOT chunk THEN
        IF NOT chunkspace^.PendingFlushCount THEN
            KeReleaseApcLock ( &chunkspace^.Lock, ipl )

            RETURN NULLPTR
        END

        // There are chunks pending flush.
        // We can flush them and grab the first freed one.

        MiFlushChunkSpace ( chunkspace )

        chunk = chunkspace^.FreeListHead
    END

    chunkspace^.FreeListHead = chunk^.Next

    KeReleaseApcLock ( &chunkspace^.Lock, ipl )

    RETURN MiVirtualAddress ( chunk )
END

FN MiFreeChunkSpace (
    IN chunkspace : ^MiChunkSpace,
    IN ptr : ^VOID,
    IN flush : UWORD,
)

    // Return a chunk to the given chunk space.

    ipl := KeAcquireApcLockExclusive ( &chunkspace^.Lock )

    IF flush THEN
        // Insert on the delayed flush table.

        count := chunkspace^.PendingFlushCount

        IF count == MI_MAXIMUM_CHUNKS_PENDING_FLUSH THEN
            // Full. Flush it.

            MiFlushChunkSpace ( chunkspace )

            count = 0
        END

        chunkspace^.PendingFlush[count] = ptr
        chunkspace^.PendingFlushCount = count + 1

    ELSE
        // Insert directly into the free chunk list.

        chunk := CAST MiPteAddress ( ptr ) TO ^MiChunkEntry

        chunk^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = chunk
    END

    KeReleaseApcLock ( &chunkspace^.Lock, ipl )
END

#ENTERSECTION "INITtext"

#SECTION "INITtext"
FN MiInitializeSystemVa ()

    // Initialize the system space allocation.

    // There are three regions of dynamically managed system space:
    //
    // MI_POOL_SPACE:     Sized at boot time with enough page tables for a
    //                    quantity of virtual space equivalent to the total
    //                    physical memory in the system.
    //
    //                    The free list for these pages is threaded through the
    //                    page tables.
    //
    //                    Maximum size on 32-bit is capped at 256MB.
    //
    //
    // MI_CACHE_SPACE:    Occupies 4x phys mem of virtual space. Page tables are
    //                    allocated at boot time. Used to map demand-paged views
    //                    of cached files. Allocated in fixed size chunks,
    //                    varying in size depending on the system size:
    //
    //                    Tiny:   32KB
    //                    *:      64KB
    //                    Large+: 256KB
    //
    //                    The free list for these chunks is threaded through the
    //                    page tables.
    //
    //                    Maximum size on 32-bit is capped at 256MB.
    //
    //
    // MI_DYNAMIC_SPACE:  Page tables are allocated at boot time. Allocations
    //                    can be any count of pages. Used to map kernel stacks,
    //                    MDLs, etc.
    //
    //                    An AVL tree is used to allocate this space. The AVL
    //                    nodes (containing parent, left child, right child, and
    //                    an optional MmObject pointer) are allocated in bulk
    //                    from nonpaged pool on-demand and stored in a lookaside
    //                    list.
    //
    //                    Maximum size on 32-bit is capped at 256MB.

    size := MiSystemPartition.SizeLevel

    KeAssert ( SIZEOF MiChunkEntry <= SIZEOF MiPte )

    // Initialize nonpaged space.

    MiInitializeChunkSpace (
        &MiPoolSpace, // chunkspace
        MI_POOL_SPACE, // base
        RTL_PAGE_SHIFT, // entryshift
        KeLoaderBlock.PoolSpaceSize, // pages
    )

    // Initialize cache space.

    entryshift : UWORD

    IF size <= MM_TINY_SYSTEM THEN
        entryshift = 15 // 1 << 15 = 32KB

    ELSEIF size < MM_LARGE_SYSTEM THEN
        entryshift = 16 // 1 << 16 = 64KB

    ELSE
        entryshift = 18 // 1 << 18 = 256KB
    END

    MiCacheSpaceChunkPages = (1 << entryshift) >> RTL_PAGE_SHIFT

    MiInitializeChunkSpace (
        &MiCacheSpace, // chunkspace
        MI_CACHE_SPACE, // base
        entryshift, // entryshift
        KeLoaderBlock.CacheSpaceSize, // pages
    )

    // Initialize dynamic space.

    RtlInitializeAvl ( &MiDynamicSpaceRoot )

    KeInitializeLock ( &MiDynamicSpaceLock )

    KeInitializeLock ( &MiDynamicSpaceTreeLock )

    // Trim enough pages off the end of dynamic space to allow each CPU to have
    // its own page for 'quick' mappings.

    dsize := KeLoaderBlock.DynamicSpaceSize

    IF dsize < KeLoaderBlock.ProcessorCount THEN
        KeCrash ( "Dynamic space too small\n" )
    END

    dsize -= KeLoaderBlock.ProcessorCount

    RtlInitializeBitmap (
        &MiDynamicSpaceBitmap, // header
        dsize, // sizeinbits
        KeLoaderBlock.DynamicSpaceBitmap, // data
    )

    // Set each processor's quick PTE.

    basepte := MiPteAddress ( MI_DYNAMIC_SPACE + (dsize << RTL_PAGE_SHIFT) )

    KeSetQuickPtes (
        basepte, // basepte
        SIZEOF MiPte, // ptesize
    )
END

#LEAVESECTION

FN MiFlushDynamicSpace ()

    // Perform the deferred flushes for dynamic space. Assumes the dynamic space
    // lock is held.

    // First free the regions.

    i := 0
    max := MiDeferredFlushIndex
    flush := &MiDeferredFlushTable[0]

    WHILE i < max DO
        RtlClearBitsBitmap (
            &MiDynamicSpaceBitmap, // header
            (flush^.Ptr - MI_DYNAMIC_SPACE) >> RTL_PAGE_SHIFT, // index
            flush^.Pages, // runlength
        )

        flush += SIZEOF MiDeferredFlush
        i += 1
    END

    MiDeferredFlushIndex = 0
    MiDeferredFlushPages = 0

    // Now do a global TB flush.

    KeSweepTb ( FALSE )
END

FN MiFindDynamicSpaceNode (
    IN ptr : ^VOID,
) : ^MiDynamicSpaceNode

    // Find the dynamic space node in which the pointer resides.
    // Returns NULLPTR if no such node exists.

    node : ^MiDynamicSpaceNode

    // We could hold this lock exclusive but that would violate our codebase
    // policy that a shared lock is never taken subordinate to an exclusive
    // lock. There are many imaginable situations where that could occur here.

    ipl := KeAcquireApcLockExclusive ( &MiDynamicSpaceTreeLock )

    avlnode := MiDynamicSpaceRoot.Right

    WHILE TRUE DO
        IF NOT avlnode THEN
            node = NULLPTR

            BREAK
        END

        node = CONTAINEROF avlnode TO MiDynamicSpaceNode.Entry

        IF ptr < node^.StartVa THEN
            avlnode = avlnode^.Left

        ELSEIF ptr >= node^.EndVa THEN
            avlnode = avlnode^.Right

        ELSE
            BREAK
        END
    END

    KeReleaseApcLock ( &MiDynamicSpaceTreeLock, ipl )

    RETURN node
END

FN MiAllocateDynamicSpaceNode () : ^MiDynamicSpaceNode

    // Allocate a dynamic space node. Assumes tree is locked exclusively.

    node := MiDynamicSpaceFreeNodeListHead

    IF node THEN
        // Got one.

        MiDynamicSpaceFreeNodeListHead = node^.Next

        RETURN CAST node TO ^MiDynamicSpaceNode
    END

    // Allocate a chunk of nodes from nonpaged pool.

    // Don't wait - let the caller drop the tree lock and then wait for
    // pages manually, then retry. Otherwise we make the tree lock held
    // across memory waits, which isn't fatal as nothing in the page-out path
    // depends on it, but is certainly inefficient.

    node = CAST MmAllocatePool (
        MM_NONPAGED_POOL, // poolindex
        MI_FREE_NODE_CHUNK_SIZE * SIZEOF MiDynamicSpaceNode, // bytes
        'Dyna', // tag
        FALSE, // wait
    ) TO ^MiDynamicSpaceFreeNode

    IF NOT node THEN
        RETURN NULLPTR
    END

    // We're going to return the first node, so start linking these into the
    // free list from the second node.

    linknode := node + SIZEOF MiDynamicSpaceNode
    i := MI_FREE_NODE_CHUNK_SIZE - 1

    WHILE i DO
        linknode^.Next = MiDynamicSpaceFreeNodeListHead
        MiDynamicSpaceFreeNodeListHead = linknode

        i -= 1
        linknode += SIZEOF MiDynamicSpaceNode
    END

    RETURN CAST node TO ^MiDynamicSpaceNode
END

FN MiFreeDynamicSpaceNode (
    IN node : ^MiDynamicSpaceNode,
)

    // Free a dynamic space node. Assumes tree is locked exclusively.

    freenode := CAST node TO ^MiDynamicSpaceFreeNode

    freenode^.Next = MiDynamicSpaceFreeNodeListHead
    MiDynamicSpaceFreeNodeListHead = freenode
END

FN (RtlAvlLessThanF) MiCompareDynamicSpaceNodes (
    IN a : ^RtlAvlNode,
    IN b : ^RtlAvlNode,
) : UWORD

    // Return whether A < B.

    node1 := CONTAINEROF a TO MiDynamicSpaceNode.Entry
    node2 := CONTAINEROF b TO MiDynamicSpaceNode.Entry

    RETURN node1^.StartVa < node2^.StartVa
END

FN MiAllocateDynamicPages (
    IN pages : UWORD,
) : ^VOID

    // Allocate a page aligned region of dynamic space.
    // Returns NULLPTR if none available.

@Retry

    // Find a clear run of bits locklessly.
    // Note that the bitmap hint is just a hint, so there's no need to do any
    // synchronization for it, since "corruption" is harmless.

    index := RtlFindRunBitmap (
        &MiDynamicSpaceBitmap, // header
        pages, // runlength
        MiDynamicSpaceHint, // hint
    )

    ipl := KeAcquireApcLockExclusive ( &MiDynamicSpaceLock )

    IF index == -1 THEN
        // Dynamic space has completely filled up.

        IF MiDeferredFlushIndex THEN
            // Try freeing the deferred regions and trying again.

            MiFlushDynamicSpace ()
        END

        // Search again with the lock held, to be absolutely sure it's full.

        index = RtlFindRunBitmap (
            &MiDynamicSpaceBitmap, // header
            pages, // runlength
            MiDynamicSpaceHint, // hint
        )

        IF index == -1 THEN
            // Yep, full.

            KeReleaseApcLock ( &MiDynamicSpaceLock, ipl )

            RETURN NULLPTR
        END
    
    ELSE
        // Check that the range is still free with the lock held.

        IF NOT RtlCheckClearBitsBitmap (
            &MiDynamicSpaceBitmap, // header
            index, // index
            pages, // runlength
        ) THEN
            // Someone else nabbed it, so retry.

            KeReleaseApcLock ( &MiDynamicSpaceLock, ipl )

            GOTO Retry
        END
    END

    // Set the hint.

    MiDynamicSpaceHint = index

    // Mark bits set.

    RtlSetBitsBitmap (
        &MiDynamicSpaceBitmap, // header
        index, // index
        pages, // runlength
    )

    KeReleaseApcLock ( &MiDynamicSpaceLock, ipl )

    RETURN CAST MI_DYNAMIC_SPACE + (index << RTL_PAGE_SHIFT) TO ^VOID
END

FN MiReleaseDynamicPages (
    IN ptr : ^VOID,
    IN pages : UWORD,
    IN flush : UWORD,
)

    // Release a portion of dynamic space by pointer and page count.

    ipl := KeAcquireApcLockExclusive ( &MiDynamicSpaceLock )

    IF flush THEN
        IF MiDeferredFlushIndex == MI_DEFERRED_FLUSH_PAIRS THEN
            // This exceeds the maximum number of regions pending flush.

            MiFlushDynamicSpace ()
        END

        // Add this region to the list of pairs pending flush.

        entry := &MiDeferredFlushTable[MiDeferredFlushIndex]

        entry^.Ptr = ptr
        entry^.Pages = pages

        MiDeferredFlushIndex += 1
        MiDeferredFlushPages += pages

        IF MiDeferredFlushPages >= MI_DEFERRED_FLUSH_PAGES THEN
            // Too many pages pending flush.

            MiFlushDynamicSpace ()
        END

    ELSE
        RtlClearBitsBitmap (
            &MiDynamicSpaceBitmap, // header
            (ptr - MI_DYNAMIC_SPACE) >> RTL_PAGE_SHIFT, // index
            pages, // runlength
        )
    END

    KeReleaseApcLock ( &MiDynamicSpaceLock, ipl )
END

FN MiAllocateDynamicSpace (
    IN pages : UWORD,
    IN wait : UWORD,
) : ^MiDynamicSpaceNode

    // Allocate a region of dynamic space and create an AVL node for lookup.

    ptr := MiAllocateDynamicPages ( pages )

    IF NOT ptr THEN
        RETURN NULLPTR
    END

@Retry

    ipl := KeAcquireApcLockExclusive ( &MiDynamicSpaceTreeLock )

    node := MiAllocateDynamicSpaceNode ()

    IF NOT node THEN
        KeReleaseApcLock ( &MiDynamicSpaceTreeLock, ipl )

        IF NOT wait THEN
            // Failed to allocate a node instantly and caller told us not to
            // wait.

            MiReleaseDynamicPages (
                ptr, // ptr
                pages, // pages
                FALSE, // flush
            )

            RETURN NULLPTR
        END

        // Caller requested wait upon being unable to allocate a node.

        MiWaitForPages (
            &MiSystemPartition, // partition
            FALSE, // low
        )

        GOTO Retry
    END

    // Insert the node in the AVL tree.

    node^.StartVa = ptr
    node^.EndVa = ptr + (pages << RTL_PAGE_SHIFT)

    RtlInsertAvl (
        &MiDynamicSpaceRoot, // root
        &node^.Entry, // node
        &MiCompareDynamicSpaceNodes, // comparisonfunc
    )

    // Unlock the tree and return the node.

    KeReleaseApcLock ( &MiDynamicSpaceTreeLock, ipl )

    RETURN node
END

FN MiReleaseDynamicSpace (
    IN node : ^MiDynamicSpaceNode,
    IN flush : UWORD,
)

    // Release a region of dynamic space by node.

    ptr := node^.StartVa
    pages := (node^.EndVa - ptr) >> RTL_PAGE_SHIFT

    ipl := KeAcquireApcLockExclusive ( &MiDynamicSpaceTreeLock )

    // Remove the AVL node from the tree.

    RtlRemoveAvl (
        &MiDynamicSpaceRoot, // root
        &node^.Entry, // node
    )

    // Free the node.

    MiFreeDynamicSpaceNode ( node )

    KeReleaseApcLock ( &MiDynamicSpaceTreeLock, ipl )

    // Free the space.

    MiReleaseDynamicPages (
        ptr, // ptr
        pages, // pages
        flush, // flush
    )
END