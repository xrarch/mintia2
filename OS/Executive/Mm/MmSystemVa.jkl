//
// Implements system address space allocation for the MINTIA Memory Manager.
//

#INCLUDE "Mmp.hjk"
#INCLUDE "../../Loader/Headers/Loader.hjk"

STRUCT MmpChunkEntry
    Next : ^MmpChunkEntry,
END

PUBLIC MmpCacheSpaceChunkPages : UWORD

#DEFINE MMP_DEFERRED_FLUSH_PAGES 128

#SECTION "INITtext"
FN MmpInitializeChunkSpace (
    IN chunkspace : ^MmpChunkSpace,
    IN base : ^VOID,
    IN entryshift : UWORD,
    IN pages : UWORD,
)

    // Initialize a chunked region of system space.

    entrysize := 1 << entryshift
    entrycount := (pages << RTL_PAGE_SHIFT) / entrysize
    entrypages := entrysize >> RTL_PAGE_SHIFT

    chunkspace^.EntryShift = entryshift
    chunkspace^.EntryCount = entrycount
    chunkspace^.FreeListHead = NULLPTR

    chunkspace^.PendingFlushCount = 0

    KeInitializeLock ( &chunkspace^.Lock )

    // Link the chunks through the PTEs.

    pte := MmpPteAddress ( base )

    WHILE entrycount DO
        entry := CAST pte TO ^MmpChunkEntry

        entry^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = entry

        entrycount -= 1
        pte += entrypages * SIZEOF MmpPte
    END
END

FN MmpFlushChunkSpace (
    IN chunkspace : ^MmpChunkSpace,
)

    // Flush TB entries across all processors for the delayed chunks.

    // Insert these entries into the free list.

    i := 0
    max := chunkspace^.PendingFlushCount

    WHILE i < max DO
        chunk := CAST MmpPteAddress ( chunkspace^.PendingFlush[i] )
            TO ^MmpChunkEntry

        chunk^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = chunk

        i += 1
    END

    chunkspace^.PendingFlushCount = 0

    // Flush the TB.

    IF chunkspace^.EntryShift == RTL_PAGE_SHIFT AND
        max < 16 THEN

        // Chunks are pages, and there's fewer than 16.
        // Flush them individually.

        KeuFlushMultipleTb (
            &chunkspace^.PendingFlush[0], // vaddrtable
            max, // pagecount
        )

    ELSE
        // Chunks are multiple pages, or there's more than 16.
        // Flush everyone's entire TB. Don't keep global pages.

        KeuSweepTb ( FALSE )
    END
END

FN MmpAllocateChunkSpace (
    IN chunkspace : ^MmpChunkSpace,
) : ^VOID

    // Allocate a chunk from the given chunk space.
    // Returns the virtual address of the chunk. Nothing is mapped into the
    // returned virtual address - this is the caller's responsibility.
    // Returns NULLPTR if no chunks remaining.

    ipl := KeAcquireApcLockExclusive ( &chunkspace^.Lock )

    chunk := chunkspace^.FreeListHead

    IF NOT chunk THEN
        IF NOT chunkspace^.PendingFlushCount THEN
            KeReleaseApcLock ( &chunkspace^.Lock, ipl )

            RETURN NULLPTR
        END

        // There are chunks pending flush.
        // We can flush them and grab the first freed one.

        MmpFlushChunkSpace ( chunkspace )

        chunk = chunkspace^.FreeListHead
    END

    chunkspace^.FreeListHead = chunk^.Next

    KeReleaseApcLock ( &chunkspace^.Lock, ipl )

    RETURN MmpVirtualAddress ( chunk )
END

FN MmpFreeChunkSpace (
    IN chunkspace : ^MmpChunkSpace,
    IN ptr : ^VOID,
    IN flush : UWORD,
)

    // Return a chunk to the given chunk space.

    ipl := KeAcquireApcLockExclusive ( &chunkspace^.Lock )

    IF flush THEN
        // Insert on the delayed flush table.

        count := chunkspace^.PendingFlushCount

        IF count == MMP_MAXIMUM_CHUNKS_PENDING_FLUSH THEN
            // Full. Flush it.

            MmpFlushChunkSpace ( chunkspace )

            count = 0
        END

        chunkspace^.PendingFlush[count] = ptr
        chunkspace^.PendingFlushCount = count + 1

    ELSE
        // Insert directly into the free chunk list.

        chunk := CAST MmpPteAddress ( ptr ) TO ^MmpChunkEntry

        chunk^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = chunk
    END

    KeReleaseApcLock ( &chunkspace^.Lock, ipl )
END

#ENTERSECTION "INITtext"

#SECTION "INITtext"
FN MmpInitializeSystemVa (
    IN node : ^MmpNode,
)

    // Initialize the system space allocation.

    // There are three regions of dynamically managed system space:
    //
    // MMP_POOL_SPACE:    Sized at boot time with enough page tables for a
    //                    quantity of virtual space equivalent to the total
    //                    physical memory in the system.
    //
    //                    The free list for these pages is threaded through the
    //                    page tables.
    //
    //                    Maximum size on 32-bit is capped at 256MB.
    //
    //
    // MMP_CACHE_SPACE:   Occupies 4x phys mem of virtual space. Page tables are
    //                    allocated at boot time. Used to map demand-paged views
    //                    of cached files. Allocated in fixed size chunks,
    //                    varying in size depending on the system size:
    //
    //                    Tiny:   32KB
    //                    *:      64KB
    //                    Large+: 256KB
    //
    //                    The free list for these chunks is threaded through the
    //                    page tables.
    //
    //                    Maximum size on 32-bit is capped at 256MB.
    //
    //
    // MMP_DYNAMIC_SPACE: Page tables are allocated at boot time. Allocations
    //                    can be any count of pages. Used to map kernel stacks,
    //                    MDLs, etc.
    //
    //                    An AVL tree is used to allocate this space. The AVL
    //                    nodes (containing parent, left child, right child, and
    //                    an optional MmuObject pointer) are allocated in bulk
    //                    from nonpaged pool on-demand and stored in a lookaside
    //                    list.
    //
    //                    Maximum size on 32-bit is capped at 256MB.

    size := node^.Partition.SizeLevel

    KeAssert ( SIZEOF MmpChunkEntry <= SIZEOF MmpPte )

    // Initialize nonpaged space.

    base := CAST MMP_POOL_SPACE + (
        (node^.Id * KeuLoaderBlock.PoolSpaceSize) << RTL_PAGE_SHIFT
    ) TO ^VOID

    MmpInitializeChunkSpace (
        &node^.PoolSpace, // chunkspace
        base, // base
        RTL_PAGE_SHIFT, // entryshift
        KeuLoaderBlock.PoolSpaceSize, // pages
    )

    // Initialize cache space.

    entryshift : UWORD

    IF size <= MM_TINY_SYSTEM THEN
        entryshift = 15 // 1 << 15 = 32KB

    ELSEIF size < MM_LARGE_SYSTEM THEN
        entryshift = 16 // 1 << 16 = 64KB

    ELSE
        entryshift = 18 // 1 << 18 = 256KB
    END

    MmpCacheSpaceChunkPages = (1 << entryshift) >> RTL_PAGE_SHIFT

    base = CAST MMP_CACHE_SPACE +
        ((node^.Id * KeuLoaderBlock.CacheSpaceSize) << RTL_PAGE_SHIFT) TO ^VOID

    MmpInitializeChunkSpace (
        &node^.CacheSpace, // chunkspace
        base, // base
        entryshift, // entryshift
        KeuLoaderBlock.CacheSpaceSize, // pages
    )

    // Initialize dynamic space.

    RtlInitializeAvl ( &node^.DynamicSpaceRoot )

    KeInitializeLock ( &node^.DynamicSpaceLock )

    KeInitializeLock ( &node^.DynamicSpaceTreeLock )

    base = CAST MMP_DYNAMIC_SPACE + (
        (node^.Id * KeuLoaderBlock.DynamicSpaceSize) << RTL_PAGE_SHIFT
    ) TO ^VOID

    node^.DynamicSpaceBase = base

    // Trim four pages off the end of dynamic space per CPU, to be used as
    // 'quick' mappings.
    //
    // XXX Currently this uses the count of *all* CPUs, not just ones in this
    //     NUMA node. This could be inefficient because it will cause extra
    //     virtual pages to be allocated for each node, but it avoids having to
    //     maintain and calculate CPU ID ranges for each NUMA node so it's fine
    //     for now.

    quickpages := KeQueryProcessorCount () * MMP_QUICK_PER_CPU

    dsize := KeuLoaderBlock.DynamicSpaceSize

    IF dsize < quickpages THEN
        KeCrash ( "Dynamic space too small\n" )
    END

    dsize -= quickpages

    RtlInitializeBitmap (
        &node^.DynamicSpaceBitmap, // header
        dsize, // sizeinbits
        KeuLoaderBlock.DynamicSpaceBitmaps[node^.Id], // data
    )

    // Remember the base of the quick pages.

    node^.QuickPteBase =
        MmpPteAddress ( base + (dsize << RTL_PAGE_SHIFT) )
END

#LEAVESECTION

#SECTION "INITtext"
FN MmpInitializeSpaceNodeAllocation (
    IN node : ^MmpNode,
)

    // Initialize the cache for dynamic space nodes.
    // This is done later as part of pool initialization because that's where
    // they ultimately come from.

    // XXX The sanity of using a pool cache for this depends on the
    //     MmpDynamicSpaceNode being small enough to not trigger page-aligned
    //     pool allocation when slabs are created for it.

    node^.SpaceNodeCache = MmCreatePoolCache (
        node, // node
        "Space Nodes", // name
        SIZEOF MmpDynamicSpaceNode, // size
        MM_NONPAGED_POOL, // poolindex
        'Dyna', // tag
    )

#ENTERSECTION "INITtext"
    IF NOT node^.SpaceNodeCache THEN
        KeCrash ( "Failed to create space node cache\n" )
    END
#LEAVESECTION "INITtext"

END

FN MmpFlushDynamicSpace (
    IN node : ^MmpNode,
)

    // Perform the deferred flushes for dynamic space. Assumes the dynamic space
    // lock is held.

    // First free the regions.

    i := 0
    max := node^.DeferredFlushIndex
    flush := &node^.DeferredFlushTable[0]
    base := node^.DynamicSpaceBase

    WHILE i < max DO
        RtlClearBitsBitmap (
            &node^.DynamicSpaceBitmap, // header
            (flush^.Ptr - base) >> RTL_PAGE_SHIFT, // index
            flush^.Pages, // runlength
        )

        flush += SIZEOF MmpDeferredFlush
        i += 1
    END

    node^.DeferredFlushIndex = 0
    node^.DeferredFlushPages = 0

    // Now do a global TB flush.

    KeuSweepTb ( FALSE )
END

FN MmpFindDynamicSpaceNode (
    IN ptr : ^VOID,
) : ^MmpDynamicSpaceNode

    // Find the dynamic space node in which the pointer resides.
    // Returns NULLPTR if no such node exists.

    node := MmpNumaNodeFromDynamicSpacePointer ( ptr )

    spacenode : ^MmpDynamicSpaceNode

    // We could hold this lock shared but that would violate our codebase
    // policy that a shared lock is never taken subordinate to an exclusive
    // lock. There are many imaginable situations where that could occur here.

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceTreeLock )

    avlnode := node^.DynamicSpaceRoot.Right

    WHILE TRUE DO
        IF NOT avlnode THEN
            spacenode = NULLPTR

            BREAK
        END

        spacenode = CONTAINEROF avlnode TO MmpDynamicSpaceNode.Entry

        IF ptr < spacenode^.StartVa THEN
            avlnode = avlnode^.Left

        ELSEIF ptr >= spacenode^.EndVa THEN
            avlnode = avlnode^.Right

        ELSE
            BREAK
        END
    END

    KeReleaseApcLock ( &node^.DynamicSpaceTreeLock, ipl )

    RETURN spacenode
END

#MACRO MmpAllocateDynamicSpaceNode ( node ) [
    CAST MmAllocateFromPoolCache (
        (node)^.SpaceNodeCache, // cache
        FALSE, // wait
    ) TO ^MmpDynamicSpaceNode
]

#MACRO MmpFreeDynamicSpaceNode ( node, spacenode ) [
    MmFreeToPoolCache (
        (node)^.SpaceNodeCache, // cache
        spacenode, // ptr
    )
]

FN (RtlAvlLessThanF) MmpCompareDynamicSpaceNodes (
    IN a : ^RtlAvlNode,
    IN b : ^RtlAvlNode,
) : UWORD

    // Return whether A < B.

    node1 := CONTAINEROF a TO MmpDynamicSpaceNode.Entry
    node2 := CONTAINEROF b TO MmpDynamicSpaceNode.Entry

    RETURN node1^.StartVa < node2^.StartVa
END

FN MmpAllocateDynamicPages (
    IN node : ^MmpNode,
    IN pages : UWORD,
) : ^VOID

    // Allocate a page aligned region of dynamic space.
    // Returns NULLPTR if none available.

@Retry

    // Find a clear run of bits locklessly.
    // Note that the bitmap hint is just a hint, so there's no need to do any
    // synchronization for it, since "corruption" is harmless.

    index := RtlFindRunBitmap (
        &node^.DynamicSpaceBitmap, // header
        pages, // runlength
        node^.DynamicSpaceHint, // hint
    )

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceLock )

    IF index == -1 THEN
        // Dynamic space has completely filled up.

        IF node^.DeferredFlushIndex THEN
            // Try freeing the deferred regions and trying again.

            MmpFlushDynamicSpace ( node )
        END

        // Search again with the lock held, to be absolutely sure it's full.

        index = RtlFindRunBitmap (
            &node^.DynamicSpaceBitmap, // header
            pages, // runlength
            node^.DynamicSpaceHint, // hint
        )

        IF index == -1 THEN
            // Yep, full.

            KeReleaseApcLock ( &node^.DynamicSpaceLock, ipl )

            RETURN NULLPTR
        END
    
    ELSE
        // Check that the range is still free with the lock held.

        IF NOT RtlCheckClearBitsBitmap (
            &node^.DynamicSpaceBitmap, // header
            index, // index
            pages, // runlength
        ) THEN
            // Someone else nabbed it, so retry.

            KeReleaseApcLock ( &node^.DynamicSpaceLock, ipl )

            GOTO Retry
        END
    END

    // Set the hint.

    node^.DynamicSpaceHint = index

    // Mark bits set.

    RtlSetBitsBitmap (
        &node^.DynamicSpaceBitmap, // header
        index, // index
        pages, // runlength
    )

    KeReleaseApcLock ( &node^.DynamicSpaceLock, ipl )

    RETURN CAST node^.DynamicSpaceBase + (index << RTL_PAGE_SHIFT) TO ^VOID
END

FN MmpReleaseDynamicPages (
    IN node : ^MmpNode,
    IN ptr : ^VOID,
    IN pages : UWORD,
    IN flush : UWORD,
)

    // Release a portion of dynamic space by pointer and page count.

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceLock )

    IF flush THEN
        IF node^.DeferredFlushIndex == MMP_DEFERRED_FLUSH_PAIRS THEN
            // This exceeds the maximum number of regions pending flush.

            MmpFlushDynamicSpace ( node )
        END

        // Add this region to the list of pairs pending flush.

        entry := &node^.DeferredFlushTable[node^.DeferredFlushIndex]

        entry^.Ptr = ptr
        entry^.Pages = pages

        node^.DeferredFlushIndex += 1
        node^.DeferredFlushPages += pages

        IF node^.DeferredFlushPages >= MMP_DEFERRED_FLUSH_PAGES THEN
            // Too many pages pending flush.

            MmpFlushDynamicSpace ( node )
        END

    ELSE
        RtlClearBitsBitmap (
            &node^.DynamicSpaceBitmap, // header
            (ptr - node^.DynamicSpaceBase) >> RTL_PAGE_SHIFT, // index
            pages, // runlength
        )
    END

    KeReleaseApcLock ( &node^.DynamicSpaceLock, ipl )
END

FN MmpAllocateDynamicSpace (
    IN node : ^MmpNode,
    IN pages : UWORD,
    IN wait : UWORD,
) : ^MmpDynamicSpaceNode

    // Allocate a region of dynamic space and create an AVL node for lookup.

    ptr := MmpAllocateDynamicPages (
        node, // node
        pages, // pages
    )

    IF NOT ptr THEN
        RETURN NULLPTR
    END

@Retry

    spacenode := MmpAllocateDynamicSpaceNode ( node )

    IF NOT spacenode THEN
        IF NOT wait THEN
            // Failed to allocate a node instantly and caller told us not to
            // wait.

            MmpReleaseDynamicPages (
                node, // node
                ptr, // ptr
                pages, // pages
                FALSE, // flush
            )

            RETURN NULLPTR
        END

        // Caller requested wait upon being unable to allocate a node.

        MmpWaitForPages (
            &node^.Partition, // partition
            FALSE, // low
        )

        GOTO Retry
    END

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceTreeLock )

    // Insert the node in the AVL tree.

    spacenode^.StartVa = ptr
    spacenode^.EndVa = ptr + (pages << RTL_PAGE_SHIFT)

    RtlInsertAvl (
        &node^.DynamicSpaceRoot, // root
        &spacenode^.Entry, // node
        &MmpCompareDynamicSpaceNodes, // comparisonfunc
    )

    // Unlock the tree and return the node.

    KeReleaseApcLock ( &node^.DynamicSpaceTreeLock, ipl )

    RETURN spacenode
END

FN MmpReleaseDynamicSpace (
    IN node : ^MmpNode,
    IN spacenode : ^MmpDynamicSpaceNode,
    IN flush : UWORD,
)

    // Release a region of dynamic space by node.

    ptr := spacenode^.StartVa
    pages := (spacenode^.EndVa - ptr) >> RTL_PAGE_SHIFT

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceTreeLock )

    // Remove the AVL node from the tree.

    RtlRemoveAvl (
        &node^.DynamicSpaceRoot, // root
        &spacenode^.Entry, // node
    )

    KeReleaseApcLock ( &node^.DynamicSpaceTreeLock, ipl )

    // Free the node.

    MmpFreeDynamicSpaceNode ( node, spacenode )

    // Free the space.

    MmpReleaseDynamicPages (
        node, // node
        ptr, // ptr
        pages, // pages
        flush, // flush
    )
END