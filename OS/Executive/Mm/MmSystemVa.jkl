//
// Implements system address space allocation for the MINTIA Memory Manager.
//

#INCLUDE "Mi.hjk"
#INCLUDE "../../Loader/Headers/Loader.hjk"

STRUCT MiChunkEntry
    Next : ^MiChunkEntry,
END

PUBLIC MiCacheSpaceChunkPages : UWORD

#DEFINE MI_DEFERRED_FLUSH_PAGES 128

#SECTION "INITtext"
FN MiInitializeChunkSpace (
    IN chunkspace : ^MiChunkSpace,
    IN base : ^VOID,
    IN entryshift : UWORD,
    IN pages : UWORD,
)

    // Initialize a chunked region of system space.

    entrysize := 1 << entryshift
    entrycount := (pages << RTL_PAGE_SHIFT) / entrysize
    entrypages := entrysize >> RTL_PAGE_SHIFT

    chunkspace^.EntryShift = entryshift
    chunkspace^.EntryCount = entrycount
    chunkspace^.FreeListHead = NULLPTR

    chunkspace^.PendingFlushCount = 0

    KeInitializeLock ( &chunkspace^.Lock )

    // Link the chunks through the PTEs.

    pte := MiPteAddress ( base )

    WHILE entrycount DO
        entry := CAST pte TO ^MiChunkEntry

        entry^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = entry

        entrycount -= 1
        pte += entrypages * SIZEOF MiPte
    END
END

FN MiFlushChunkSpace (
    IN chunkspace : ^MiChunkSpace,
)

    // Flush TB entries across all processors for the delayed chunks.

    // Insert these entries into the free list.

    i := 0
    max := chunkspace^.PendingFlushCount

    WHILE i < max DO
        chunk := CAST MiPteAddress ( chunkspace^.PendingFlush[i] )
            TO ^MiChunkEntry

        chunk^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = chunk

        i += 1
    END

    chunkspace^.PendingFlushCount = 0

    // Flush the TB.

    IF chunkspace^.EntryShift == RTL_PAGE_SHIFT AND
        max < 16 THEN

        // Chunks are pages, and there's fewer than 16.
        // Flush them individually.

        KeFlushMultipleTb (
            &chunkspace^.PendingFlush[0], // vaddrtable
            max, // pagecount
        )

    ELSE
        // Chunks are multiple pages, or there's more than 16.
        // Flush everyone's entire TB. Don't keep global pages.

        KeSweepTb ( FALSE )
    END
END

FN MiAllocateChunkSpace (
    IN chunkspace : ^MiChunkSpace,
) : ^VOID

    // Allocate a chunk from the given chunk space.
    // Returns the virtual address of the chunk. Nothing is mapped into the
    // returned virtual address - this is the caller's responsibility.
    // Returns NULLPTR if no chunks remaining.

    ipl := KeAcquireApcLockExclusive ( &chunkspace^.Lock )

    chunk := chunkspace^.FreeListHead

    IF NOT chunk THEN
        IF NOT chunkspace^.PendingFlushCount THEN
            KeReleaseApcLock ( &chunkspace^.Lock, ipl )

            RETURN NULLPTR
        END

        // There are chunks pending flush.
        // We can flush them and grab the first freed one.

        MiFlushChunkSpace ( chunkspace )

        chunk = chunkspace^.FreeListHead
    END

    chunkspace^.FreeListHead = chunk^.Next

    KeReleaseApcLock ( &chunkspace^.Lock, ipl )

    RETURN MiVirtualAddress ( chunk )
END

FN MiFreeChunkSpace (
    IN chunkspace : ^MiChunkSpace,
    IN ptr : ^VOID,
    IN flush : UWORD,
)

    // Return a chunk to the given chunk space.

    ipl := KeAcquireApcLockExclusive ( &chunkspace^.Lock )

    IF flush THEN
        // Insert on the delayed flush table.

        count := chunkspace^.PendingFlushCount

        IF count == MI_MAXIMUM_CHUNKS_PENDING_FLUSH THEN
            // Full. Flush it.

            MiFlushChunkSpace ( chunkspace )

            count = 0
        END

        chunkspace^.PendingFlush[count] = ptr
        chunkspace^.PendingFlushCount = count + 1

    ELSE
        // Insert directly into the free chunk list.

        chunk := CAST MiPteAddress ( ptr ) TO ^MiChunkEntry

        chunk^.Next = chunkspace^.FreeListHead
        chunkspace^.FreeListHead = chunk
    END

    KeReleaseApcLock ( &chunkspace^.Lock, ipl )
END

#ENTERSECTION "INITtext"

#SECTION "INITtext"
FN MiInitializeSystemVa (
    IN node : ^MiNode,
)

    // Initialize the system space allocation.

    // There are three regions of dynamically managed system space:
    //
    // MI_POOL_SPACE:     Sized at boot time with enough page tables for a
    //                    quantity of virtual space equivalent to the total
    //                    physical memory in the system.
    //
    //                    The free list for these pages is threaded through the
    //                    page tables.
    //
    //                    Maximum size on 32-bit is capped at 256MB.
    //
    //
    // MI_CACHE_SPACE:    Occupies 4x phys mem of virtual space. Page tables are
    //                    allocated at boot time. Used to map demand-paged views
    //                    of cached files. Allocated in fixed size chunks,
    //                    varying in size depending on the system size:
    //
    //                    Tiny:   32KB
    //                    *:      64KB
    //                    Large+: 256KB
    //
    //                    The free list for these chunks is threaded through the
    //                    page tables.
    //
    //                    Maximum size on 32-bit is capped at 256MB.
    //
    //
    // MI_DYNAMIC_SPACE:  Page tables are allocated at boot time. Allocations
    //                    can be any count of pages. Used to map kernel stacks,
    //                    MDLs, etc.
    //
    //                    An AVL tree is used to allocate this space. The AVL
    //                    nodes (containing parent, left child, right child, and
    //                    an optional MmObject pointer) are allocated in bulk
    //                    from nonpaged pool on-demand and stored in a lookaside
    //                    list.
    //
    //                    Maximum size on 32-bit is capped at 256MB.

    size := node^.Partition.SizeLevel

    KeAssert ( SIZEOF MiChunkEntry <= SIZEOF MiPte )

    // Initialize nonpaged space.

    base := CAST MI_POOL_SPACE +
        ((node^.Id * KeLoaderBlock.PoolSpaceSize) << RTL_PAGE_SHIFT) TO ^VOID

    MiInitializeChunkSpace (
        &node^.PoolSpace, // chunkspace
        base, // base
        RTL_PAGE_SHIFT, // entryshift
        KeLoaderBlock.PoolSpaceSize, // pages
    )

    // Initialize cache space.

    entryshift : UWORD

    IF size <= MM_TINY_SYSTEM THEN
        entryshift = 15 // 1 << 15 = 32KB

    ELSEIF size < MM_LARGE_SYSTEM THEN
        entryshift = 16 // 1 << 16 = 64KB

    ELSE
        entryshift = 18 // 1 << 18 = 256KB
    END

    MiCacheSpaceChunkPages = (1 << entryshift) >> RTL_PAGE_SHIFT

    base = CAST MI_CACHE_SPACE +
        ((node^.Id * KeLoaderBlock.CacheSpaceSize) << RTL_PAGE_SHIFT) TO ^VOID

    MiInitializeChunkSpace (
        &node^.CacheSpace, // chunkspace
        base, // base
        entryshift, // entryshift
        KeLoaderBlock.CacheSpaceSize, // pages
    )

    // Initialize dynamic space.

    RtlInitializeAvl ( &node^.DynamicSpaceRoot )

    KeInitializeLock ( &node^.DynamicSpaceLock )

    KeInitializeLock ( &node^.DynamicSpaceTreeLock )

    base = CAST MI_DYNAMIC_SPACE +
        ((node^.Id * KeLoaderBlock.DynamicSpaceSize) << RTL_PAGE_SHIFT) TO ^VOID

    node^.DynamicSpaceBase = base

    // Trim four pages off the end of dynamic space per CPU, to be used as
    // 'quick' mappings.
    //
    // XXX Currently this uses the count of *all* CPUs, not just ones in this
    //     NUMA node. This could be inefficient because it will cause extra
    //     virtual pages to be allocated for each node, but it avoids having to
    //     maintain and calculate CPU ID ranges for each NUMA node so it's fine
    //     for now.

    quickpages := KeQueryProcessorCount () * MI_QUICK_PER_CPU

    dsize := KeLoaderBlock.DynamicSpaceSize

    IF dsize < quickpages THEN
        KeCrash ( "Dynamic space too small\n" )
    END

    dsize -= quickpages

    RtlInitializeBitmap (
        &node^.DynamicSpaceBitmap, // header
        dsize, // sizeinbits
        KeLoaderBlock.DynamicSpaceBitmaps[node^.Id], // data
    )

    // Remember the base of the quick pages.

    node^.QuickPteBase =
        MiPteAddress ( base + (dsize << RTL_PAGE_SHIFT) )
END

#LEAVESECTION

#SECTION "INITtext"
FN MiInitializeSpaceNodeAllocation (
    IN node : ^MiNode,
)

    // Initialize the cache for dynamic space nodes.
    // This is done later as part of pool initialization because that's where
    // they ultimately come from.

    // XXX The sanity of using a pool cache for this depends on the
    //     MiDynamicSpaceNode being small enough to not trigger page-aligned
    //     pool allocation when slabs are created for it.

    node^.SpaceNodeCache = MmCreatePoolCache (
        node, // node
        "Space Nodes", // name
        SIZEOF MiDynamicSpaceNode, // size
        MM_NONPAGED_POOL, // poolindex
        'Dyna', // tag
        NULLPTR, // constructor
        NULLPTR, // destructor
        NULLPTR, // context
    )

#ENTERSECTION "INITtext"
    IF NOT node^.SpaceNodeCache THEN
        KeCrash ( "Failed to create space node cache\n" )
    END
#LEAVESECTION "INITtext"

END

FN MiFlushDynamicSpace (
    IN node : ^MiNode,
)

    // Perform the deferred flushes for dynamic space. Assumes the dynamic space
    // lock is held.

    // First free the regions.

    i := 0
    max := node^.DeferredFlushIndex
    flush := &node^.DeferredFlushTable[0]
    base := node^.DynamicSpaceBase

    WHILE i < max DO
        RtlClearBitsBitmap (
            &node^.DynamicSpaceBitmap, // header
            (flush^.Ptr - base) >> RTL_PAGE_SHIFT, // index
            flush^.Pages, // runlength
        )

        flush += SIZEOF MiDeferredFlush
        i += 1
    END

    node^.DeferredFlushIndex = 0
    node^.DeferredFlushPages = 0

    // Now do a global TB flush.

    KeSweepTb ( FALSE )
END

FN MiFindDynamicSpaceNode (
    IN ptr : ^VOID,
) : ^MiDynamicSpaceNode

    // Find the dynamic space node in which the pointer resides.
    // Returns NULLPTR if no such node exists.

    node := MiNumaNodeFromDynamicSpacePointer ( ptr )

    spacenode : ^MiDynamicSpaceNode

    // We could hold this lock shared but that would violate our codebase
    // policy that a shared lock is never taken subordinate to an exclusive
    // lock. There are many imaginable situations where that could occur here.

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceTreeLock )

    avlnode := node^.DynamicSpaceRoot.Right

    WHILE TRUE DO
        IF NOT avlnode THEN
            spacenode = NULLPTR

            BREAK
        END

        spacenode = CONTAINEROF avlnode TO MiDynamicSpaceNode.Entry

        IF ptr < spacenode^.StartVa THEN
            avlnode = avlnode^.Left

        ELSEIF ptr >= spacenode^.EndVa THEN
            avlnode = avlnode^.Right

        ELSE
            BREAK
        END
    END

    KeReleaseApcLock ( &node^.DynamicSpaceTreeLock, ipl )

    RETURN spacenode
END

#MACRO MiAllocateDynamicSpaceNode ( node ) [
    CAST MmAllocateFromPoolCache (
        (node)^.SpaceNodeCache, // cache
        FALSE, // wait
    ) TO ^MiDynamicSpaceNode
]

#MACRO MiFreeDynamicSpaceNode ( node, spacenode ) [
    MmFreeToPoolCache (
        (node)^.SpaceNodeCache, // cache
        spacenode, // ptr
    )
]

FN (RtlAvlLessThanF) MiCompareDynamicSpaceNodes (
    IN a : ^RtlAvlNode,
    IN b : ^RtlAvlNode,
) : UWORD

    // Return whether A < B.

    node1 := CONTAINEROF a TO MiDynamicSpaceNode.Entry
    node2 := CONTAINEROF b TO MiDynamicSpaceNode.Entry

    RETURN node1^.StartVa < node2^.StartVa
END

FN MiAllocateDynamicPages (
    IN node : ^MiNode,
    IN pages : UWORD,
) : ^VOID

    // Allocate a page aligned region of dynamic space.
    // Returns NULLPTR if none available.

@Retry

    // Find a clear run of bits locklessly.
    // Note that the bitmap hint is just a hint, so there's no need to do any
    // synchronization for it, since "corruption" is harmless.

    index := RtlFindRunBitmap (
        &node^.DynamicSpaceBitmap, // header
        pages, // runlength
        node^.DynamicSpaceHint, // hint
    )

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceLock )

    IF index == -1 THEN
        // Dynamic space has completely filled up.

        IF node^.DeferredFlushIndex THEN
            // Try freeing the deferred regions and trying again.

            MiFlushDynamicSpace ( node )
        END

        // Search again with the lock held, to be absolutely sure it's full.

        index = RtlFindRunBitmap (
            &node^.DynamicSpaceBitmap, // header
            pages, // runlength
            node^.DynamicSpaceHint, // hint
        )

        IF index == -1 THEN
            // Yep, full.

            KeReleaseApcLock ( &node^.DynamicSpaceLock, ipl )

            RETURN NULLPTR
        END
    
    ELSE
        // Check that the range is still free with the lock held.

        IF NOT RtlCheckClearBitsBitmap (
            &node^.DynamicSpaceBitmap, // header
            index, // index
            pages, // runlength
        ) THEN
            // Someone else nabbed it, so retry.

            KeReleaseApcLock ( &node^.DynamicSpaceLock, ipl )

            GOTO Retry
        END
    END

    // Set the hint.

    node^.DynamicSpaceHint = index

    // Mark bits set.

    RtlSetBitsBitmap (
        &node^.DynamicSpaceBitmap, // header
        index, // index
        pages, // runlength
    )

    KeReleaseApcLock ( &node^.DynamicSpaceLock, ipl )

    RETURN CAST node^.DynamicSpaceBase + (index << RTL_PAGE_SHIFT) TO ^VOID
END

FN MiReleaseDynamicPages (
    IN node : ^MiNode,
    IN ptr : ^VOID,
    IN pages : UWORD,
    IN flush : UWORD,
)

    // Release a portion of dynamic space by pointer and page count.

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceLock )

    IF flush THEN
        IF node^.DeferredFlushIndex == MI_DEFERRED_FLUSH_PAIRS THEN
            // This exceeds the maximum number of regions pending flush.

            MiFlushDynamicSpace ( node )
        END

        // Add this region to the list of pairs pending flush.

        entry := &node^.DeferredFlushTable[node^.DeferredFlushIndex]

        entry^.Ptr = ptr
        entry^.Pages = pages

        node^.DeferredFlushIndex += 1
        node^.DeferredFlushPages += pages

        IF node^.DeferredFlushPages >= MI_DEFERRED_FLUSH_PAGES THEN
            // Too many pages pending flush.

            MiFlushDynamicSpace ( node )
        END

    ELSE
        RtlClearBitsBitmap (
            &node^.DynamicSpaceBitmap, // header
            (ptr - node^.DynamicSpaceBase) >> RTL_PAGE_SHIFT, // index
            pages, // runlength
        )
    END

    KeReleaseApcLock ( &node^.DynamicSpaceLock, ipl )
END

FN MiAllocateDynamicSpace (
    IN node : ^MiNode,
    IN pages : UWORD,
    IN wait : UWORD,
) : ^MiDynamicSpaceNode

    // Allocate a region of dynamic space and create an AVL node for lookup.

    ptr := MiAllocateDynamicPages (
        node, // node
        pages, // pages
    )

    IF NOT ptr THEN
        RETURN NULLPTR
    END

@Retry

    spacenode := MiAllocateDynamicSpaceNode ( node )

    IF NOT spacenode THEN
        IF NOT wait THEN
            // Failed to allocate a node instantly and caller told us not to
            // wait.

            MiReleaseDynamicPages (
                node, // node
                ptr, // ptr
                pages, // pages
                FALSE, // flush
            )

            RETURN NULLPTR
        END

        // Caller requested wait upon being unable to allocate a node.

        MiWaitForPages (
            &node^.Partition, // partition
            FALSE, // low
        )

        GOTO Retry
    END

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceTreeLock )

    // Insert the node in the AVL tree.

    spacenode^.StartVa = ptr
    spacenode^.EndVa = ptr + (pages << RTL_PAGE_SHIFT)

    RtlInsertAvl (
        &node^.DynamicSpaceRoot, // root
        &spacenode^.Entry, // node
        &MiCompareDynamicSpaceNodes, // comparisonfunc
    )

    // Unlock the tree and return the node.

    KeReleaseApcLock ( &node^.DynamicSpaceTreeLock, ipl )

    RETURN spacenode
END

FN MiReleaseDynamicSpace (
    IN node : ^MiNode,
    IN spacenode : ^MiDynamicSpaceNode,
    IN flush : UWORD,
)

    // Release a region of dynamic space by node.

    ptr := spacenode^.StartVa
    pages := (spacenode^.EndVa - ptr) >> RTL_PAGE_SHIFT

    ipl := KeAcquireApcLockExclusive ( &node^.DynamicSpaceTreeLock )

    // Remove the AVL node from the tree.

    RtlRemoveAvl (
        &node^.DynamicSpaceRoot, // root
        &spacenode^.Entry, // node
    )

    KeReleaseApcLock ( &node^.DynamicSpaceTreeLock, ipl )

    // Free the node.

    MiFreeDynamicSpaceNode ( node, spacenode )

    // Free the space.

    MiReleaseDynamicPages (
        node, // node
        ptr, // ptr
        pages, // pages
        flush, // flush
    )
END