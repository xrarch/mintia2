//
// Implements quick-mapping.
//

#INCLUDE "Mmp.hjk"

STRUCT MmpQuickPteBlock
    Locks : KeLock[MMP_QUICK_PER_CPU],
    BasePte : ^MmpPte,
    NextIndex : UBYTE,
END

#DEFINE MMP_QUICK_PTE_BLOCK_SIZE [((SIZEOF MmpQuickPteBlock + KE_CACHE_ALIGN - 1) &
    ~(KE_CACHE_ALIGN - 1))]

#ENTERSECTION "INITtext"

#SECTION "INITtext"
FN MmpInitializeQuickPages (
    IN node : ^MmpNode,
)

    // Initialize the quick page mappings.

    procs := KeQueryProcessorCount ()

    node^.QuickPteBlocks = MmAllocatePool (
        MM_NONPAGED_POOL, // poolindex
        MMP_QUICK_PTE_BLOCK_SIZE * procs, // bytes
        'Quic', // tag
        FALSE, // wait
    )

    IF NOT node^.QuickPteBlocks THEN
        KeCrash ( "Failed to allocate quick PTE blocks\n" )
    END

    i := 0

    WHILE i < procs DO
        block := node^.QuickPteBlocks + (i * MMP_QUICK_PTE_BLOCK_SIZE)

        block^.NextIndex = 0

        block^.BasePte = node^.QuickPteBase +
            ((i * MMP_QUICK_PER_CPU) * SIZEOF MmpPte)

        j := 0

        WHILE j < MMP_QUICK_PER_CPU DO
            KeInitializeLock ( &block^.Locks[j] )

            j += 1
        END

        i += 1

    END
END

#LEAVESECTION

FN MmUseQuickPte (
    IN func : MmUseQuickPteF,
    IN pfn : UWORD,
    IN context : ^VOID,
) : OsStatus

    // Allocate a quick mapping for the given PFN, then call the callback with
    // a virtual address that we mapped the PFN to.

    thread := KeCurrentThread ()

    node := PsCurrentNode ()

    // Mask KAPCs. Otherwise one could be dispatched while we've claimed a quick
    // PTE, and then a complicated self-deadlock could occur below.

    ipl := KeMaskApcs ()

    // Disable migration of the current thread to another processor.
    // This will make it so we don't need to do a TB shootdown below.

    old := KeuControlMigration (
        thread, // thread
        TRUE, // pinned
    )

    id := KeCurrentProcessorId ()

    // Trylock all of this processor's quick PTEs.

    block := node^.QuickPteBlocks + (id * MMP_QUICK_PTE_BLOCK_SIZE)

    i := 0

    WHILE i < MMP_QUICK_PER_CPU DO
        IF KeTryAcquireLockExclusive ( &block^.Locks[i] ) THEN
            GOTO GotOne
        END

        i += 1
    END

    // Failed to trylock a page. Increment a counter and lock whatever
    // it lands on.

    i = block^.NextIndex & (MMP_QUICK_PER_CPU - 1)
    block^.NextIndex = i + 1

    // Lock the quick page.

    KeAcquireLockExclusive ( &block^.Locks[i] )

@GotOne

    // Calculate the PTE and virtual address.

    pte := block^.BasePte + (i * SIZEOF MmpPte)
    vaddr := MmpVirtualAddress ( pte )

    // Map the page.

    pte[0] = MmpBuildPoolPte ( pfn )

    // Flush TB. We only need to flush the local TB because we disabled
    // migration of the current thread to any other CPU.

    KeuFlushMyTbAddress ( vaddr )

    // Call provided function.

    status := func (
        vaddr, // vaddr
        context, // context
    )

    // Release the page lock.

    KeReleaseLock ( &block^.Locks[i] )

    // Restore migration.

    KeuControlMigration (
        thread, // thread
        old, // pinned
    )

    // Restore KAPCs.

    KeUnmaskApcs ( ipl )

    RETURN status
END

FN (MmUseQuickPteF) MmpZeroPageFunc (
    IN vaddr : ^VOID,
    IN context : ^VOID,
) : OsStatus

    // Called with the pointer to a quick mapping to zero the page through.

    RtlFillMemoryWithUlong (
        vaddr, // ptr
        RTL_PAGE_SIZE, // sz
        0, // ulong
    )
END

FN MmZeroPage (
    IN pfn : UWORD,
)

    // Zero out a page frame.

    MmUseQuickPte (
        &MmpZeroPageFunc, // func
        pfn, // pfn
        NULLPTR, // context
    )
END