//
// Various architecture-specific functions.
//

#INCLUDE "../Kep.hjk"
#INCLUDE "../../Mm/xr17032/Mmp.hjk"

#ASM [

// a0 - abortblock
// a1 - func
// a2 - context
// a3 - flags
// outputs:
// a3 - status
KeExceptionJacket:
.export KeExceptionJacket

    // Set up an abort state for the current thread.

    subi sp, sp, 12
    mov  long [sp + 8], lr
    mov  long [sp + 4], s0
    mov  long [sp], s1

    mov  s0, a0

    // Acquire a pointer to the current thread.

    subi t0, zero, KEP_PRB_LESS_ZERO
    mov  s1, long [t0 + KepPrb_CurrentThread]

    // Bias the current thread pointer by 4 * ExecutingKapc.
    // This makes it so when we add the offset KeuThread_AbortListHead later,
    // we point to the right list head.

    mov  t0, byte [s1 + KeuThread_ExecutingKapc]
    add  s1, s1, t0 LSH 2

    // Initialize the abort block.

    mov  long [s0 + (KeAbortBlock_Context + OsContext_Sp)], sp
    mov  long [s0 + KeAbortBlock_Flags], a3

    // Save all the non-volatile (callee-saved) registers that we didn't already
    // save on our stack frame.

    mov  long [s0 + (KeAbortBlock_Context + OsContext_S2)], s2
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S3)], s3
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S4)], s4
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S5)], s5
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S6)], s6
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S7)], s7
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S8)], s8
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S9)], s9
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S10)], s10
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S11)], s11
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S12)], s12
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S13)], s13
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S14)], s14
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S15)], s15
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S16)], s16
    mov  long [s0 + (KeAbortBlock_Context + OsContext_S17)], s17

    // Link it into the thread. No sync needed because we only access the abort
    // list from the context of this thread.

    mov  t1, long [s1 + KeuThread_AbortListHead]
    mov  long [s0 + KeAbortBlock_Next], t1
    mov  long [s1 + KeuThread_AbortListHead], s0

    // Call the function.

    mov  a0, a2
    jalr lr, a1, 0

.return:

    // Unlink the abort block.

    mov  t0, long [s0 + KeAbortBlock_Next]
    mov  long [s1 + KeuThread_AbortListHead], t0

    // Return normally.

    mov  s1, long [sp]
    mov  s0, long [sp + 4]
    mov  lr, long [sp + 8]
    addi sp, sp, 12

    ret

// s0 - abortblock
// s1 - thread
KepAbortVector:
.global KepAbortVector

    // We get vectored here when an abort occurred. We have to restore state
    // from the abort block.

    mov  sp, long [s0 + (KeAbortBlock_Context + OsContext_Sp)]

    // Load the status the caller should receive.

    mov  a3, long [s0 + KeAbortBlock_Status]

    // Restore non-volatile state that wasn't saved on the stack frame.

    mov  s2, long [s0 + (KeAbortBlock_Context + OsContext_S2)]
    mov  s3, long [s0 + (KeAbortBlock_Context + OsContext_S3)]
    mov  s4, long [s0 + (KeAbortBlock_Context + OsContext_S4)]
    mov  s5, long [s0 + (KeAbortBlock_Context + OsContext_S5)]
    mov  s6, long [s0 + (KeAbortBlock_Context + OsContext_S6)]
    mov  s7, long [s0 + (KeAbortBlock_Context + OsContext_S7)]
    mov  s8, long [s0 + (KeAbortBlock_Context + OsContext_S8)]
    mov  s9, long [s0 + (KeAbortBlock_Context + OsContext_S9)]
    mov  s10, long [s0 + (KeAbortBlock_Context + OsContext_S10)]
    mov  s11, long [s0 + (KeAbortBlock_Context + OsContext_S11)]
    mov  s12, long [s0 + (KeAbortBlock_Context + OsContext_S12)]
    mov  s13, long [s0 + (KeAbortBlock_Context + OsContext_S13)]
    mov  s14, long [s0 + (KeAbortBlock_Context + OsContext_S14)]
    mov  s15, long [s0 + (KeAbortBlock_Context + OsContext_S15)]
    mov  s16, long [s0 + (KeAbortBlock_Context + OsContext_S16)]
    mov  s17, long [s0 + (KeAbortBlock_Context + OsContext_S17)]

    b    KeExceptionJacket.return

// outputs:
// a3 - old interrupt state
KepDisableInterrupts:
.global KepDisableInterrupts

    mfcr a3, rs                     // Load current value of RS.
    subi t0, zero, 3                // t0 = 0xFFFFFFFD
    and  t0, a3, t0                 // Mask out the INT bit.
    mtcr rs, t0                     // Write new RS.

    ret

// a0 - old interrupt state
KepRestoreInterrupts:
.global KepRestoreInterrupts

    mtcr rs, a0                     // Write new RS.

    ret

KepEnableInterrupts:
.global KepEnableInterrupts

    mfcr t0, rs
    ori  t0, t0, 2
    mtcr rs, t0

    ret

// Debugger doorstop
.dl 0

KepWaitForInterrupt:
.global KepWaitForInterrupt

    hlt

    ret

// outputs:
// a3 - processor id
KepCurrentProcessor:
.global KepCurrentProcessor

    mfcr a3, whami                  // Load WHO-AM-I control register.

    ret

// outputs:
// a3 - current ipl
KepCurrentIpl:
.global KepCurrentIpl

    subi t0, zero, KEP_PRB_LESS_ZERO // Calculate Prb address.
    mov  a3, byte [t0 + KepPrb_Ipl]  // Load IPL.

    ret

// a0 - ptr
// a1 - inc
// outputs:
// a3 - old value
KeIncrementUlong:
.export KeIncrementUlong

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

.retry:
    ll   a3, a0                     // Load-locked the location.
    add  t1, a3, a1                 // Add the increment.
    sc   t1, a0, t1                 // Conditionally store the new value.
    beq  t1, .retry                 // If store failed, retry.

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

    ret

// a0 - ptr
// a1 - newvalue
// a2 - expectedvalue
// outputs:
// a3 - origvalue
KeCompareSwapUlong:
.export KeCompareSwapUlong

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

.retry:
    ll   a3, a0                     // Load-locked the location.
    sub  t0, a3, a2                 // Compare equality with expected value.
    bne  t0, .exit                  // Not equal, leave.
    sc   t0, a0, a1                 // Conditionally store the new value.
    beq  t0, .retry                 // If store failed, retry.

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

.exit:

    ret

// a0 - ptr
// a1 - newvalue
// a2 - expectedvalue
// outputs:
// a3 - origvalue
// a2 - aborted
KeCompareSwapUlongAbortOnFault:
.export KeCompareSwapUlongAbortOnFault

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

KepStartAbortCasRegion:
.global KepStartAbortCasRegion

.retry:
    ll   a3, a0                     // Load-locked the location.
    sub  t0, a3, a2                 // Compare equality with expected value.
    bne  t0, KepExitAbortCas         // Not equal, leave.
    sc   t0, a0, a1                 // Conditionally store the new value.
    beq  t0, .retry                 // If store failed, retry.

KepEndAbortCasRegion:
.global KepEndAbortCasRegion

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

KepExitAbortCas:

    li   a2, 0

    ret

KepAbortCas:
.global KepAbortCas

    // We took a page fault while attempting the CAS. Return something that
    // is guaranteed not to be equal to the expectedvalue, thereby making the
    // CAS look like a failure.

    addi a3, a2, 1
    li   a2, 1

    ret

// a0 - ptr
// a1 - mask
// outputs:
// a3 - origvalue
KeMaskUlong:
.export KeMaskUlong

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

.retry:
    ll   a3, a0                     // Load-locked the location.
    and  t0, a3, a1                 // Mask the value.
    sc   t0, a0, t0                 // Conditionally store the new value.
    beq  t0, .retry                 // If store failed, retry.

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

    ret

// a0 - ptr
// a1 - bitset
// outputs:
// a3 - origvalue
KeOrUlong:
.export KeOrUlong

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

.retry:
    ll   a3, a0                     // Load-locked the location.
    or   t0, a3, a1                 // OR the value.
    sc   t0, a0, t0                 // Conditionally store the new value.
    beq  t0, .retry                 // If store failed, retry.

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

    ret

// a0 - ptr
// a1 - newvalue
// outputs:
// a3 - origvalue
KeFetchAndSetUlong:
.export KeFetchAndSetUlong

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

.retry:
    ll   a3, a0                     // Load-locked the location.
    sc   t0, a0, a1                 // Conditionally set the location.
    beq  t0, .retry                 // If store failed, retry.

#IF BLD_MP
    mb                              // Ensure coherence with other processors.
#END

    ret

// Debugger doorstop
.dl 0

KeBreakpoint:
.export KeBreakpoint
    
    subi sp, sp, 4
    mov  long [sp], lr

    brk

    mov  lr, long [sp]
    addi sp, sp, 4

    ret

KepFlushMyIcache:
.global KepFlushMyIcache
    
    li   t0, 3
    mtcr icachectrl, t0

    ret

KepFlushMyDcache:
.global KepFlushMyDcache
    
    li   t0, 3
    mtcr dcachectrl, t0

    ret

// inputs:
// a0 - keepglobal
KeuSweepMyTb:
.global KeuSweepMyTb

    beq  a0, .flushglobal

    li   t0, 2
    mtcr itbctrl, t0
    mtcr dtbctrl, t0

    ret

.flushglobal:

    li   t0, 1
    mtcr itbctrl, t0
    mtcr dtbctrl, t0

    ret


// inputs:
// a0 - address
KeuFlushMyTbAddress:
.global KeuFlushMyTbAddress

    // Ensure zeroes in the low 12 bits.

    subi t0, zero, 4096
    and  a0, a0, t0

    mtcr itbctrl, a0                // Flush the ITB page.
    mtcr dtbctrl, a0                // Flush the DTB page.

    ret

// inputs:
// a0 - pfn
KepFlushMyIcacheByPhys:
.global KepFlushMyIcacheByPhys

    // Flush a physical page from the icache.

    lshi a0, a0, 12

    // OR the command bits in.

    ori  a0, a0, 2

    // Flush the page.

    mtcr icachectrl, a0

    ret

// inputs:
// a0 - pfn
KepFlushMyDcacheByPhys:
.global KepFlushMyDcacheByPhys

    // Flush a physical page from the dcache.

    lshi a0, a0, 12

    // OR the command bits in.

    ori  a0, a0, 2

    // Flush the page.

    mtcr dcachectrl, a0

    ret

]

#IF BLD_MP

FN (KepIpiF) KepSweepTb (
    IN context1 : UWORD,
    IN context2 : UWORD,
)

    // Executes on the target processor of a TB shootdown.

    KeuSweepMyTb ( context1 )
END

FN (KepIpiF) KepFlushSingleTb (
    IN context1 : UWORD,
    IN context2 : UWORD,
)

    // Executes on the target processor of a TB shootdown.

    KeuFlushMyTbAddress ( CAST context1 TO ^VOID )
END

FN (KepIpiF) KepFlushMultipleTb (
    IN context1 : UWORD,
    IN context2 : UWORD,
)

    // Executes on the target processor of a TB shootdown.

    vaddrtable := CAST context1 TO ^^VOID

    WHILE context2 DO
        KeuFlushMyTbAddress ( vaddrtable^ )

        vaddrtable += SIZEOF ^VOID
        context2 -= 1
    END
END

FN (KepIpiF) KepSweepDcache (
    IN context1 : UWORD,
    IN context2 : UWORD,
)

    // Executes on the target processor of a Dcache shootdown.

    KepFlushMyDcache ()
END

FN (KepIpiF) KepFlushMultipleDcache (
    IN context1 : UWORD,
    IN context2 : UWORD,
)

    // Executes on the target processor of a Dcache shootdown.

    pfntable := CAST context1 TO ^^VOID

    WHILE context2 DO
        KepFlushMyDcacheByPhys ( pfntable^ )

        pfntable += SIZEOF ^VOID
        context2 -= 1
    END
END

FN (KepIpiF) KepSweepIcache (
    IN context1 : UWORD,
    IN context2 : UWORD,
)

    // Executes on the target processor of an Icache shootdown.

    KepFlushMyIcache ()
END

FN (KepIpiF) KepFlushMultipleIcache (
    IN context1 : UWORD,
    IN context2 : UWORD,
)

    // Executes on the target processor of an Icache shootdown.

    pfntable := CAST context1 TO ^^VOID

    WHILE context2 DO
        KepFlushMyIcacheByPhys ( pfntable^ )

        pfntable += SIZEOF ^VOID
        context2 -= 1
    END
END

#END

FN KeuSweepTb (
    IN keepglobal : UWORD,
)

    // Flush the entire TB on all processors.

#IF BLD_MP
    ipl := KepRaiseIpl ( KEP_IPL_DPC )

    KepExecuteOnEveryoneElse (
        &KepSweepTb, // function
        keepglobal, // context1
        0, // context2
        KEP_DEFERRED_IDLE_TB_SHOOTDOWN, // deferred
    )

    KepLowerIpl ( ipl )
#END

    KeuSweepMyTb ( keepglobal )
END

FN KeuFlushSingleTb (
    IN vaddr : ^VOID,
)

    // Flush a single TB entry on all processors.

#IF BLD_MP
    ipl := KepRaiseIpl ( KEP_IPL_DPC )

    KepExecuteOnEveryoneElse (
        &KepFlushSingleTb, // function
        vaddr, // context1
        0, // context2
        KEP_DEFERRED_IDLE_TB_SHOOTDOWN, // deferred
    )

    KepLowerIpl ( ipl )
#END

    KeuFlushMyTbAddress ( vaddr )
END

FN KeuFlushMultipleTb (
    IN vaddrtable : ^^VOID,
    IN pagecount : UWORD,
)

    // Flush multiple TB entries on all processors.

#IF BLD_MP
    ipl := KepRaiseIpl ( KEP_IPL_DPC )

    KepExecuteOnEveryoneElse (
        &KepFlushMultipleTb, // function
        vaddrtable, // context1
        pagecount, // context2
        KEP_DEFERRED_IDLE_TB_SHOOTDOWN, // deferred
    )

    KepLowerIpl ( ipl )
#END

    WHILE pagecount DO
        KeuFlushMyTbAddress ( vaddrtable^ )

        vaddrtable += SIZEOF ^VOID
        pagecount -= 1
    END
END

FN KeuSweepDcache ()

    // Flush the entire Dcache on all processors.

#IF BLD_MP
    ipl := KepRaiseIpl ( KEP_IPL_DPC )

    KepExecuteOnEveryoneElse (
        &KepSweepDcache, // function
        0, // context1
        0, // context2
        KEP_DEFERRED_IDLE_DCACHE_SHOOTDOWN, // deferred
    )

    KepLowerIpl ( ipl )
#END

    KepFlushMyDcache ()
END

FN KeuFlushDcachePages (
    IN pfntable : ^^VOID,
    IN pagecount : UWORD,
)

    // Flush pages of the Dcache on all processors.

#IF BLD_MP
    ipl := KepRaiseIpl ( KEP_IPL_DPC )

    KepExecuteOnEveryoneElse (
        &KepFlushMultipleDcache, // function
        pfntable, // context1
        pagecount, // context2
        KEP_DEFERRED_IDLE_DCACHE_SHOOTDOWN, // deferred
    )

    KepLowerIpl ( ipl )
#END

    WHILE pagecount DO
        KepFlushMyDcacheByPhys ( pfntable^ )

        pfntable += SIZEOF ^VOID
        pagecount -= 1
    END
END

FN KeuSweepIcache ()

    // Flush the entire Icache on all processors.

#IF BLD_MP
    ipl := KepRaiseIpl ( KEP_IPL_DPC )

    KepExecuteOnEveryoneElse (
        &KepSweepIcache, // function
        0, // context1
        0, // context2
        KEP_DEFERRED_IDLE_ICACHE_SHOOTDOWN, // deferred
    )

    KepLowerIpl ( ipl )
#END

    KepFlushMyIcache ()
END

FN KeuFlushIcachePages (
    IN pfntable : ^^VOID,
    IN pagecount : UWORD,
)

    // Flush pages of the Icache on all processors.

#IF BLD_MP
    ipl := KepRaiseIpl ( KEP_IPL_DPC )

    KepExecuteOnEveryoneElse (
        &KepFlushMultipleIcache, // function
        pfntable, // context1
        pagecount, // context2
        KEP_DEFERRED_IDLE_ICACHE_SHOOTDOWN, // deferred
    )

    KepLowerIpl ( ipl )
#END

    WHILE pagecount DO
        KepFlushMyIcacheByPhys ( pfntable^ )

        pfntable += SIZEOF ^VOID
        pagecount -= 1
    END
END