//
// Implements spinlocks for the kernel.
//

#INCLUDE "../Kep.hjk"

#IF BLD_MP

#ASM [

// a0 - spinlock
KepForceReleaseSpinlock:
.global KepForceReleaseSpinlock

    mb

    mov  long [a0], zero

    ret

// a0 - spinlock
// outputs:
// a3 - success
KepTryAcquireSpinlock:
.global KepTryAcquireSpinlock

    // Initialize the try spin count.

    li  t1, 128

.retry:
    ll  t0, a0
    bne t0, .spin
    sc  t0, a0, a0
    beq t0, .retry

    // Memory barrier to ensure reads in the critical section don't see stale
    // junk.

    mb

    li   a3, 1
    ret

    // Spin until free without using atomics since they may incur more cache
    // coherency traffic.

.spin:
    pause
    mov  t0, long [a0]
    beq  t0, .retry

    // Decrement the try count and spin again if we have more tries left.

    subi t1, t1, 1
    bne  t1, .spin

    // No tries left, report failure.

    li   a3, 0
    ret

// a0 - offset
// outputs:
// a3 - oldipl
// a2 - prb
KepAcquireSpinlockInPrb:
.global KepAcquireSpinlockInPrb

    // Note that we access the Prb through a per-processor mapping via the wired
    // TB entries on XR17032. Therefore we can get a pointer to it before we
    // raise IPL and that's safe, allowing us to order it with the following
    // fall-through.
    
    subi a0, a0, KEP_PRB_LESS_ZERO   // Calculate the pointer of the spinlock
                                    // within the Prb.

    // Fall through.

// a0 - spinlock
// outputs:
// a3 - oldipl
KepAcquireSpinlockRaise:
.global KepAcquireSpinlockRaise

    // Inline KepRaiseIpl here.

    subi a2, zero, KEP_PRB_LESS_ZERO // Acquire a pointer to the Prb.
    mov  a3, byte [a2 + KepPrb_Ipl]  // Load the old IPL.
    mov  byte [a2 + KepPrb_Ipl], 2   // Store the new IPL.

    // Fall through.

// a0 - spinlock
KepAcquireSpinlock:
.global KepAcquireSpinlock

    // Common case: non-contended.

.retry:
    ll   t0, a0
    bne  t0, .spin
    sc   t0, a0, a0
    beq  t0, .spin

    // Memory barrier to ensure reads in the critical section don't see stale
    // junk.

    mb

    ret

    // Spin until free without using atomics since they may incur more cache
    // coherency traffic.

.spin:
    pause
    mov  t0, long [a0]
    bne  t0, .spin
    b    .retry

]

#END