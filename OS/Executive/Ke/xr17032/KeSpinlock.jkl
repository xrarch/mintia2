//
// Implements spinlocks for the kernel.
//

#INCLUDE "../Ki.hjk"

#IF BLD_MP

#ASM [

// a0 - spinlock
KiForceReleaseSpinlock:
.global KiForceReleaseSpinlock

    mb

    mov  long [a0], zero

    ret

// a0 - spinlock
// outputs:
// a3 - success
KiTryAcquireSpinlock:
.global KiTryAcquireSpinlock

    // Initialize the try spin count.

    li  t1, 128

.retry:
    ll  t0, a0
    bne t0, .spin
    sc  t0, a0, a0
    beq t0, .retry

    // Memory barrier to ensure reads in the critical section don't see stale
    // junk.

    mb

    li   a3, 1
    ret

    // Spin until free without using atomics since they may incur more cache
    // coherency traffic.

.spin:
    pause
    mov  t0, long [a0]
    beq  t0, .retry

    // Decrement the try count and spin again if we have more tries left.

    subi t1, t1, 1
    bne  t1, .spin

    // No tries left, report failure.

    li   a3, 0
    ret

// a0 - offset
// outputs:
// a3 - oldipl
// a2 - prb
KiAcquireSpinlockInPrb:
.global KiAcquireSpinlockInPrb

    // Note that we access the Prb through a per-processor mapping via the wired
    // TB entries on XR17032. Therefore we can get a pointer to it before we
    // raise IPL and that's safe, allowing us to order it with the following
    // fall-through.
    
    subi a0, a0, 4096               // Calculate the pointer of the spinlock
                                    // within the Prb.

    // Fall through.

// a0 - spinlock
// outputs:
// a3 - oldipl
KiAcquireSpinlockRaise:
.global KiAcquireSpinlockRaise

    // Inline KiRaiseIpl here.

    subi a2, zero, 4096             // Acquire a pointer to the Prb.
    mov  a3, byte [a2 + KiPrb_Ipl]  // Load the old IPL.
    mov  byte [a2 + KiPrb_Ipl], 2   // Store the new IPL.

    // Fall through.

// a0 - spinlock
KiAcquireSpinlock:
.global KiAcquireSpinlock

    // Common case: non-contended.

.retry:
    ll   t0, a0
    bne  t0, .spin
    sc   t0, a0, a0
    beq  t0, .spin

    // Memory barrier to ensure reads in the critical section don't see stale
    // junk.

    mb

    ret

    // Spin until free without using atomics since they may incur more cache
    // coherency traffic.

.spin:
    pause
    mov  t0, long [a0]
    bne  t0, .spin
    b    .retry

]

#END